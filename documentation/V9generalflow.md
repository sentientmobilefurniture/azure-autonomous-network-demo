# V9 â€” Config-Driven Multi-Agent Orchestration

## Why This Comes First

Before adding Fabric or any new backend, the platform needs to be genericized. Right now data schemas, agent tools/instructions, graph ingestion, and the provisioner all have hardcoded assumptions tied to the telco-noc scenario. If we add Fabric on top of that, we'd have to retrofit genericization around two hardcoded backends instead of one. Do the abstraction first, then new backends slot in cleanly.

## Core Idea

A single config file (YAML or similar) fully describes a deployment â€” the agents, their data sources, the storage backends, the query languages, the prompts, the inter-agent connections, and the UI bindings. The platform reads this config and wires everything together automatically. The current telco-noc scenario becomes just one config file that happens to use cosmosdb-gremlin + specific prompts. A new scenario = a new config file + data, zero code changes.

## The Flow

### 1. Data Exists Somewhere

- User either manually generates data for a scenario, or gets an LLM to do it. The platform doesn't generate it.
- Data can live in CosmosDB, Blob Storage, or (later) Fabric.
- Data also includes prompts (system prompts, runbooks, etc.).

### 2. Named Connectors / Adapters

- Each backend gets a named adapter: e.g. `cosmosdb-gremlin`, `blob-json`, `fabric-kql`, `ai-search`, etc.
- These are format+target-location specific â€” they know how to read/write/query their backend.
- They are referred to by name in the config file.
- When an agent is assigned a data source using connector X, it automatically gets:
  - The right query language injected into its system prompt (Gremlin, KQL, SQL, etc.)
  - The schema/ontology/metadata for that specific dataset
  - The correct API endpoints / connection details

### 3. Agent Definitions in Config

- Each agent is declared with:
  - Name and model
  - Instructions / system prompt (which may reference prompt files stored as data)
  - Which data sources it uses (by connector name + specific dataset/container/table)
  - Which other agents it connects to (for multi-agent handoffs)
  - Any scenario-specific metadata
- Because an agent is assigned to specific data, it also receives that data's metadata â€” ontology, table structure, index fields, graph schema, whatever applies.

### 4. Platform Handles the Wiring

- Reads the config â†’ provisions agents in Azure AI Foundry
- Binds the right tools (OpenAPI specs, code interpreter, etc.)
- Sets up inter-agent connections (ConnectedAgentTool)
- Routes graph/topology/telemetry data to the correct UI components
- Injects query language guidance + schema metadata into agent prompts automatically

### 5. Config Is Persisted

- Stored somewhere accessible at runtime (Blob, CosmosDB, local file, etc.)
- The app can reconstruct the full topology without re-provisioning
- When you provision agents or retrieve a graph topology, the app ensures the right data goes to the right agent and the right UI element

## What This Means Architecturally

- `graph-query-api` becomes a thin router that delegates to the right adapter based on config
- `agent_provisioner.py` becomes a generic engine that reads agent specs from config
- The frontend reads topology/metadata from config rather than hardcoded routes
- Every scenario-specific hardcoding gets removed
- The current telco-noc scenario is just one example config

## Config Schema â€” Key Sections (Draft)

```yaml
scenario:
  name: "telco-noc"
  description: "Telecom NOC monitoring and incident response"

data_sources:
  - name: "network-graph"
    connector: "cosmosdb-gremlin"
    config:
      database: "..."
      container: "..."
    schema:
      # ontology / vertex-edge types / properties
    metadata:
      query_language: "gremlin"
      # anything the agent needs to know about querying this source

  - name: "runbooks"
    connector: "blob-json"
    config:
      container: "..."
      path_prefix: "..."

  - name: "telemetry"
    connector: "ai-search"
    config:
      index_name: "..."
    metadata:
      query_language: "odata"
      fields: [...]

agents:
  - name: "noc-orchestrator"
    model: "gpt-4o"
    instructions_file: "prompts/orchestrator.md"
    data_sources: ["network-graph", "runbooks"]
    connected_agents: ["graph-analyst", "telemetry-analyst"]

  - name: "graph-analyst"
    model: "gpt-4o"
    instructions_file: "prompts/graph-analyst.md"
    data_sources: ["network-graph"]
    tools:
      - type: "openapi"
        spec: "openapi/graph.yaml"

  - name: "telemetry-analyst"
    model: "gpt-4o"
    instructions_file: "prompts/telemetry-analyst.md"
    data_sources: ["telemetry"]
    tools:
      - type: "openapi"
        spec: "openapi/telemetry.yaml"
```

## Hard Parts / Considerations

1. **Config schema expressiveness** â€” needs to be powerful enough to capture all wiring without becoming its own programming language. Keep it declarative.

2. **Adapter abstraction** â€” each backend has very different query patterns (Gremlin traversals vs KQL vs SQL vs REST). The adapter interface needs to be generic enough to cover all of them while still being useful.

3. **Prompt templating** â€” injecting schema metadata + query language guidance dynamically into agent system prompts. Need a templating approach (Jinja2, string interpolation, etc.) that composes base instructions + connector-specific guidance + dataset-specific metadata.

4. **UI rendering** â€” the frontend must be able to render arbitrary topologies, not just the current telco-noc layout. Force-graph is already flexible, but the sidebar, panels, and data display need to be config-aware.

5. **OpenAPI spec generation** â€” if each connector exposes different query endpoints, the OpenAPI specs given to agents may need to be generated or templated per data source, not hand-written per scenario.

6. **Migration path** â€” the current telco-noc scenario should keep working throughout this refactor. Build the generic layer, then migrate telco-noc to use it as the first "config-driven" scenario.

## Sequencing

1. **Audit** â€” catalog every hardcoded assumption, every scenario-specific reference, every place that needs to become config-driven
2. **Design the config schema** â€” nail down the YAML structure
3. **Build adapter abstraction** â€” define the interface, implement cosmosdb-gremlin as first adapter
4. **Generic agent provisioner** â€” reads agent specs from config, provisions in Foundry
5. **Generic graph-query-api** â€” thin router delegating to adapters
6. **Prompt templating** â€” auto-inject query language + schema into agent prompts
7. **Frontend genericization** â€” config-aware rendering
8. **Migrate telco-noc** â€” rewrite current scenario as a config file, verify everything works
9. **Add new backends** â€” Fabric, Neo4j, etc. become new adapters + config options

---

## Codebase Audit â€” Layer by Layer

### Layer 1: graph-query-api (Port 8100)

The data-plane micro-service. All `/query/*` routes. This is the most complex layer.

#### Files
- `main.py` â€” FastAPI app, mounts 7 routers, SSE log streaming
- `config.py` â€” env var loading, `GraphBackendType` enum, `ScenarioContext` dataclass
- `models.py` â€” Pydantic models for requests/responses (graph, telemetry, topology, interactions)
- `backends/__init__.py` â€” `GraphBackend` Protocol, factory `get_backend()`, per-graph cache
- `backends/cosmosdb.py` â€” CosmosDB Gremlin backend (304 lines)
- `backends/mock.py` â€” Mock backend for offline demos
- `cosmos_helpers.py` â€” Singleton CosmosClient, ARM container creation, container cache
- `router_graph.py` â€” `POST /query/graph` â€” dispatches to GraphBackend
- `router_topology.py` â€” `POST /query/topology` â€” returns nodes/edges for force-graph UI
- `router_telemetry.py` â€” `POST /query/telemetry` â€” Cosmos SQL against NoSQL containers
- `router_ingest.py` â€” **872 lines** â€” uploads graph, telemetry, runbooks, tickets, prompts as tarballs
- `router_prompts.py` â€” CRUD for agent prompts in Cosmos NoSQL (289 lines)
- `router_scenarios.py` â€” CRUD for scenario metadata in Cosmos NoSQL
- `router_interactions.py` â€” Save/list/get/delete investigation records
- `search_indexer.py` â€” Creates AI Search indexes from blob containers
- `sse_helpers.py` â€” SSE progress helper for uploads
- `openapi/cosmosdb.yaml` â€” OpenAPI spec template (Gremlin-specific descriptions)
- `openapi/mock.yaml` â€” OpenAPI spec template (mock-specific descriptions)

#### What's Already Generic âœ…
- **GraphBackend Protocol** â€” `backends/__init__.py` defines a clean interface with `execute_query()`, `get_topology()`, `close()`. CosmosDB and Mock implement it.
- **Per-graph backend cache** â€” `get_backend_for_context(ctx)` routes to the right backend instance per scenario.
- **ScenarioContext** â€” `config.py` derives per-request routing from `X-Graph` header (graph name, telemetry DB, prompts container, telemetry prefix). This is already scenario-aware.
- **graph_schema.yaml driven ingestion** â€” `router_ingest.py` reads vertex/edge definitions from YAML and loads generically. No entity-type hardcoding in the loader itself.
- **scenario.yaml manifest** â€” Each scenario has a manifest with paths, cosmos config, search indexes, graph styles. The ingest router reads this.

#### What's Hardcoded / Needs Genericization ðŸ”´
1. **`GraphBackendType` enum** is `cosmosdb | mock` only. Adding Fabric requires extending this, OR better: removing the enum entirely and using string-based adapter names loaded from config.
2. **Backend factory** (`get_backend()`, `get_backend_for_graph()`) â€” dispatches via if/elif. Should become registry-based or plugin-based.
3. **`BACKEND_REQUIRED_VARS`** in config.py â€” maps backend type to required env vars. Hardcoded per backend.
4. **`router_telemetry.py`** â€” entirely Cosmos SQL-specific. Hardcodes `CosmosClient`, `query_items()`. A Fabric backend would need KQL instead. This router would need to dispatch to a telemetry adapter.
5. **`router_ingest.py`** â€” massively hardcoded to Cosmos:
   - Graph upload calls `_gremlin_client()`, `_gremlin_submit()` directly
   - Telemetry upload calls `get_cosmos_client()`, `upsert_item()` directly
   - Knowledge files upload to Azure Blob + AI Search directly
   - `_ensure_gremlin_graph()` uses ARM API specific to Cosmos Gremlin
   - `_ensure_nosql_containers()` uses ARM API specific to Cosmos NoSQL
   - `PROMPT_AGENT_MAP` hardcodes agent filename â†’ role mapping
6. **OpenAPI specs** â€” `openapi/cosmosdb.yaml` contains Gremlin-specific descriptions ("Execute a Gremlin query..."). A Fabric backend would need KQL-specific descriptions. These should be generated/templated per connector.
7. **`OPENAPI_SPEC_MAP`** in `agent_provisioner.py` â€” maps `cosmosdb â†’ cosmosdb.yaml`, `mock â†’ mock.yaml`. No Fabric entry.
8. **`cosmos_helpers.py`** â€” all Cosmos-specific. ARM container creation, CosmosClient singleton. Fabric would have its own helpers.
9. **Telemetry container prefixing** â€” `{prefix}-AlertStream` pattern is hardcoded in both `router_telemetry.py` and `ScenarioContext`.
10. **Prompts database** â€” hardcoded to Cosmos NoSQL `prompts` DB. The prompt CRUD is Cosmos-specific.

#### Key Observation
The **graph backend** (query + topology) already has a decent abstraction. The bigger problem is that **telemetry**, **ingestion**, **prompts**, and **scenario CRUD** are all tightly coupled to Cosmos NoSQL. These need adapter abstractions too â€” or at minimum, the current Cosmos implementation needs to be wrapped so a Fabric implementation can slot in.

---

### Layer 2: API Backend (Port 8000)

The orchestrator/agent-facing API. All `/api/*` routes.

#### Files
- `api/app/main.py` â€” FastAPI app, mounts 4 routers
- `api/app/orchestrator.py` â€” **501 lines** â€” bridges Foundry agent streaming to SSE
- `api/app/routers/alert.py` â€” `POST /api/alert` â€” submits alert, returns SSE stream
- `api/app/routers/agents.py` â€” `GET /api/agents` â€” lists agents from `agent_ids.json`
- `api/app/routers/config.py` â€” `POST /api/config/apply` + `GET /api/config/current` â€” re-provisions agents
- `api/app/routers/logs.py` â€” `GET /api/logs` â€” SSE log streaming

#### What's Already Generic âœ…
- **`orchestrator.py`** â€” runs ANY orchestrator agent by ID. Reads agent_ids.json, resolves names. No scenario-specific logic in the streaming bridge.
- **`alert.py`** â€” passes text to orchestrator, returns SSE. Scenario-agnostic.
- **`agents.py`** â€” reads agent_ids.json. Scenario-agnostic (though stub names are telco-specific).

#### What's Hardcoded / Needs Genericization ðŸ”´
1. **`config.py` `/api/config/apply`** â€” hardcodes exactly 5 agent types (`orchestrator`, `graph_explorer`, `telemetry`, `runbook`, `ticket`). Default prompts are hardcoded. The provisioning call assumes this exact agent structure.
2. **Stub agent names** in `agents.py` â€” `["Orchestrator", "GraphExplorerAgent", "TelemetryAgent", "RunbookKBAgent", "HistoricalTicketAgent"]`. These are telco-NOC-specific role names.
3. **Config state** â€” `_current_config` tracks `graph`, `runbooks_index`, `tickets_index`. This assumes exactly these data sources exist.
4. **Prompt fetching in config.py** â€” calls `http://127.0.0.1:8100/query/prompts?scenario=X` and maps by agent name. The mapping assumes fixed agent roles.
5. **Search connection ID** â€” hardcoded path pattern `aisearch-connection`.

#### Key Observation
The API layer is relatively thin and mostly generic. The main issue is `config.py` which assumes a fixed 5-agent topology. Making the agent set configurable is the core change needed here.

---

### Layer 3: Agent Provisioner (scripts/)

Creates Azure AI Foundry agents with tools and inter-agent connections.

#### Files
- `scripts/agent_provisioner.py` â€” **282 lines** â€” `AgentProvisioner` class
- `scripts/provision_agents.py` â€” CLI wrapper, loads prompts from disk

#### What's Hardcoded / Needs Genericization ðŸ”´
1. **`AGENT_NAMES`** â€” `["GraphExplorerAgent", "TelemetryAgent", "RunbookKBAgent", "HistoricalTicketAgent", "Orchestrator"]`. Fixed 5-agent structure.
2. **`OPENAPI_SPEC_MAP`** â€” `cosmosdb â†’ cosmosdb.yaml`, `mock â†’ mock.yaml`. No extensibility.
3. **`GRAPH_TOOL_DESCRIPTIONS`** â€” backend-specific descriptions. Hardcoded per backend.
4. **`provision_all()`** â€” creates exactly 4 sub-agents + 1 orchestrator in a hardcoded sequence:
   - GraphExplorer â†’ OpenAPI tool (graph query)
   - Telemetry â†’ OpenAPI tool (telemetry query)
   - RunbookKB â†’ AzureAISearchTool (runbooks index)
   - HistoricalTicket â†’ AzureAISearchTool (tickets index)
   - Orchestrator â†’ ConnectedAgentTool to all 4 above
5. **`provision_agents.py`** â€” loads prompts from fixed filenames: `foundry_orchestrator_agent.md`, `foundry_telemetry_agent_v2.md`, etc.
6. **`_load_graph_explorer_prompt()`** â€” composes from `core_instructions.md`, `core_schema.md`, `language_gremlin.md`. The composition pattern is hardcoded.

#### Key Observation
This is the most rigidly structured layer. The entire concept of "5 agents with these specific roles and tools" needs to become declarative. A config file should define N agents, each with their tools (OpenAPI, AzureAISearch, CodeInterpreter, ConnectedAgent), and the provisioner should create them generically.

---

### Layer 4: Data Layer (data/scenarios/)

#### Structure per scenario (telco-noc)
```
data/scenarios/telco-noc/
â”œâ”€â”€ scenario.yaml          # manifest: name, description, cosmos config, paths, graph_styles
â”œâ”€â”€ graph_schema.yaml      # vertex/edge definitions for Gremlin ingestion
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ entities/          # CSV files for graph vertices (DimCoreRouter.csv, etc.)
â”‚   â”œâ”€â”€ telemetry/         # AlertStream.csv, LinkTelemetry.csv
â”‚   â”œâ”€â”€ knowledge/
â”‚   â”‚   â”œâ”€â”€ runbooks/      # .md files â†’ Blob + AI Search
â”‚   â”‚   â””â”€â”€ tickets/       # .txt files â†’ Blob + AI Search
â”‚   â””â”€â”€ prompts/           # Agent prompt .md files
â”‚       â””â”€â”€ graph_explorer/ # Composable prompt fragments
â””â”€â”€ scripts/               # Generation scripts (not used at runtime)
```

#### What's Already Generic âœ…
- **`scenario.yaml`** â€” already declarative: name, display_name, description, paths, cosmos config, search indexes, graph_styles, use_cases, example_questions.
- **`graph_schema.yaml`** â€” fully generic vertex/edge loader. Any entity types work. No telco-specific code in the loader.
- **Tarball upload** â€” each data type (graph, telemetry, runbooks, tickets, prompts) uploads separately as `.tar.gz`. The upload router reads `scenario.yaml` to resolve names.

#### What's Hardcoded / Needs Genericization ðŸ”´
1. **`scenario.yaml` cosmos section** â€” assumes `gremlin.database`, `gremlin.graph`, `nosql.database`, `nosql.containers`. A Fabric scenario would need `kusto.cluster`, `kusto.database`, `kusto.tables` instead.
2. **`graph_schema.yaml`** â€” Gremlin-specific concept (vertices, edges, addV/addE). A Fabric equivalent would be table definitions. The ingest logic in `router_ingest.py` is entirely Gremlin.
3. **Prompt filenames** â€” `PROMPT_AGENT_MAP` in `router_ingest.py` hardcodes the mapping: `foundry_orchestrator_agent.md â†’ orchestrator`, etc. New agent roles would need new entries.
4. **GraphExplorer prompt composition** â€” hardcodes looking for `graph_explorer/core_instructions.md`, `core_schema.md`, `language_gremlin.md`.
5. **CSV-based ingestion** â€” telemetry assumes CSV â†’ Cosmos NoSQL `upsert_item()`. A Fabric backend might need CSV â†’ KQL `.ingest inline`.

#### Key Observation
The data layer is actually the best organized. `scenario.yaml` and `graph_schema.yaml` are already close to what a generic config system needs. The main gap is making `scenario.yaml` backend-agnostic â€” supporting multiple data store types, not just CosmosDB Gremlin + NoSQL.

---

### Layer 5: Frontend (React + TypeScript)

#### Key Files
- `context/ScenarioContext.tsx` â€” global state for active scenario, graph, indexes, styles
- `hooks/useTopology.ts` â€” fetches topology from `/query/topology`
- `hooks/useNodeColor.ts` â€” resolves node colors (user override â†’ scenario â†’ hardcoded â†’ auto)
- `hooks/useInvestigation.ts` â€” submits alerts, handles SSE streaming
- `hooks/useScenarios.ts` â€” manages scenario list from `/query/scenarios/saved`
- `hooks/useInteractions.ts` â€” interaction history CRUD
- `components/graph/graphConstants.ts` â€” **hardcoded** NODE_COLORS and NODE_SIZES
- `components/GraphTopologyViewer.tsx` â€” force-graph rendering
- `components/SettingsModal.tsx` â€” data source/prompt configuration UI
- `components/AddScenarioModal.tsx` â€” scenario upload UI
- `types/index.ts` â€” TypeScript types for SavedScenario, StepEvent, Interaction

#### What's Already Generic âœ…
- **`ScenarioContext`** â€” already derives bindings from scenario name (`{name}-topology`, `{name}-runbooks-index`, etc.)
- **`useTopology`** â€” fetches from `/query/topology` with `X-Graph` header. Scenario-agnostic.
- **`useNodeColor`** â€” fallback chain: user override â†’ scenario-driven â†’ hardcoded â†’ auto-hash. The auto-hash means NEW label types get colors automatically.
- **`SavedScenario` type** â€” supports arbitrary `resources`, `graph_styles`, `use_cases`, `example_questions`.
- **Force-graph rendering** â€” renders arbitrary node/edge topologies. Not scenario-specific.

#### What's Hardcoded / Needs Genericization ðŸ”´
1. **`graphConstants.ts`** â€” hardcoded `NODE_COLORS` and `NODE_SIZES` for `CoreRouter`, `AggSwitch`, `BaseStation`, `TransportLink`, `MPLSPath`, `Service`, `SLAPolicy`, `BGPSession`. These are telco-specific labels.
2. **`SettingsModal`** â€” likely assumes specific data source types (graph, runbooks, tickets).
3. **Agent stub names** â€” `_STUB_AGENTS` in `alert.py` references telco agent names.
4. **Resource derivation** â€” `ScenarioContext` hardcodes the pattern `{name}-topology`, `{name}-runbooks-index`, `{name}-tickets-index`. This assumes every scenario has exactly these resources.

#### Key Observation
The frontend is surprisingly well-prepared. The force-graph renders any topology, `useNodeColor` auto-assigns colors for unknown labels, and `ScenarioContext` already manages per-scenario state. The main changes are: (a) remove hardcoded `graphConstants.ts` defaults (or keep as fallback), (b) make `SettingsModal` data-source-aware from config, (c) make resource derivation configurable rather than pattern-based.

---

### Layer 6: Config & Deployment

#### Files
- `azure_config.env.template` â€” environment variables (all scenarios share one config)
- `Dockerfile` â€” single container: nginx + API + graph-query-api
- `supervisord.conf` â€” runs nginx, API, graph-query-api
- `nginx.conf` â€” routes `/api/*` â†’ :8000, `/query/*` â†’ :8100, `/` â†’ React SPA
- `azure.yaml` â€” Azure Developer CLI config
- `deploy.sh` â€” deployment helper
- `infra/` â€” Bicep templates for Azure resources

#### What's Already Generic âœ…
- **Container architecture** â€” 3-service single container is scenario-agnostic.
- **nginx routing** â€” path-based, no scenario-specific routes.
- **Bicep** â€” creates shared Cosmos DBs (`networkgraph`, `telemetry`, `prompts`, `scenarios`, `interactions`), AI Search, Storage. Shared resources, not per-scenario.

#### What's Hardcoded / Needs Genericization ðŸ”´
1. **`azure_config.env.template`** â€” has `DEFAULT_SCENARIO=telco-noc` and `LOADED_SCENARIOS=telco-noc`. These should become runtime config.
2. **`COSMOS_GREMLIN_GRAPH=topology`** â€” default graph name. Should come from scenario config.
3. **`RUNBOOKS_INDEX_NAME=runbooks-index`**, **`TICKETS_INDEX_NAME=tickets-index`** â€” default index names. Should come from scenario config.

#### Key Observation
Deployment layer is mostly clean. The few hardcoded defaults in `.env.template` are minor â€” they're only used as fallbacks when no scenario is selected.

---

## Summary: What Must Change (Priority Order)

### Must Change (Blockers for Genericization)
1. **Agent provisioner** â€” from hardcoded 5 agents to config-driven N agents with arbitrary tools
2. **Telemetry router** â€” from Cosmos SQL-only to adapter-dispatched
3. **Ingest router** â€” from Cosmos Gremlin-only to adapter-dispatched ingestion
4. **OpenAPI specs** â€” from static per-backend YAML to generated/templated per connector+scenario
5. **Backend enum** â€” from `cosmosdb | mock` to extensible registry
6. **Config apply endpoint** â€” from hardcoded 5-agent provisioning to config-driven

### Should Change (Clean but Not Blocking)
7. **`graphConstants.ts`** â€” remove hardcoded telco colors/sizes (keep auto-assign)
8. **Prompt filename mapping** â€” from hardcoded `PROMPT_AGENT_MAP` to config-driven
9. **ScenarioContext resource derivation** â€” from pattern-based to config-specified
10. **Stub agents** â€” remove telco-specific stub names

### Already Good (Minimal or No Change)
- GraphBackend Protocol + factory
- ScenarioContext multi-graph support
- scenario.yaml / graph_schema.yaml structure
- Force-graph topology rendering
- Orchestrator SSE bridge
- nginx / Dockerfile / supervisord architecture
---

## Codebase Investigation â€” Concrete Details for Implementation

> This section captures exact code shapes, interface contracts, and parameters
> discovered during codebase audit. Intent: remove guesswork from implementation.

### 1. Exact GraphBackend Protocol (Current Interface)

From `graph-query-api/backends/__init__.py`:

```python
@runtime_checkable
class GraphBackend(Protocol):
    async def execute_query(self, query: str, **kwargs) -> dict:
        """Returns {columns: [{name, type}], data: [dict]}"""
        ...

    async def get_topology(
        self,
        query: str | None = None,
        vertex_labels: list[str] | None = None,
    ) -> dict:
        """Returns {nodes: [{id, label, properties}], edges: [{id, source, target, label, properties}]}"""
        ...

    def close(self) -> None: ...
```

**V9 implication:** This Protocol is query-language-agnostic already â€” `execute_query(query: str)` accepts any string. The adapter decides how to interpret it (Gremlin, KQL, SQL, etc.). No change needed to the Protocol itself. The `**kwargs` allows passing extra parameters per-backend.

**Gap:** The telemetry query path does NOT use `GraphBackend`. It's a separate code path directly in `router_telemetry.py` calling `_execute_cosmos_sql()`. For V9, telemetry needs its own adapter protocol or the existing `GraphBackend` needs a telemetry method, or a separate `TelemetryBackend` protocol.

### 2. Backend Factory & Cache (Exact Dispatch Pattern)

From `graph-query-api/backends/__init__.py`:

```python
_backend_cache: dict[str, GraphBackend] = {}
_backend_lock = threading.Lock()

def get_backend_for_context(ctx: ScenarioContext) -> GraphBackend:
    if ctx.backend_type == GraphBackendType.MOCK:
        return get_backend_for_graph("__mock__", ctx.backend_type)
    return get_backend_for_graph(ctx.graph_name, ctx.backend_type)

def get_backend_for_graph(graph_name, backend_type=None) -> GraphBackend:
    bt = backend_type or GRAPH_BACKEND
    cache_key = f"{bt.value}:{graph_name}"
    with _backend_lock:
        if cache_key not in _backend_cache:
            if bt == GraphBackendType.COSMOSDB:
                from .cosmosdb import CosmosDBGremlinBackend
                _backend_cache[cache_key] = CosmosDBGremlinBackend(graph_name=graph_name)
            elif bt == GraphBackendType.MOCK:
                from .mock import MockGraphBackend
                _backend_cache[cache_key] = MockGraphBackend()
            else:
                raise ValueError(...)
```

**V9 plan:** Replace `if/elif` dispatch with a registry pattern. Each adapter registers itself by name. The factory looks up by string key from config.

### 3. ScenarioContext â€” Full Current Shape

From `graph-query-api/config.py`:

```python
@dataclass
class ScenarioContext:
    graph_name: str                  # "cloud-outage-topology"
    gremlin_database: str            # "networkgraph" (shared)
    telemetry_database: str          # "telemetry" (shared)
    telemetry_container_prefix: str  # "cloud-outage"
    prompts_database: str            # "prompts" (shared)
    prompts_container: str           # "cloud-outage"
    backend_type: GraphBackendType
```

**V9 implication:** This dataclass needs extension. A config-driven scenario would add:
- `connector_name: str` â€” which adapter to use (replaces `backend_type` enum)
- Per-connector config dict (connection strings, database names, etc.)
- Currently, derivation is hardcoded: `graph_name.rsplit("-", 1)[0]` â†’ prefix. Config should make this explicit.

### 4. Agent Provisioner â€” Exact Hardcoded Structure

From `scripts/agent_provisioner.py`:

```python
AGENT_NAMES = [
    "GraphExplorerAgent", "TelemetryAgent", "RunbookKBAgent",
    "HistoricalTicketAgent", "Orchestrator",
]

OPENAPI_SPEC_MAP = {
    "cosmosdb": OPENAPI_DIR / "cosmosdb.yaml",
    "mock": OPENAPI_DIR / "mock.yaml",
}

GRAPH_TOOL_DESCRIPTIONS = {
    "cosmosdb": "Execute a Gremlin query against Azure Cosmos DB...",
    "mock": "Query the topology graph (offline mock mode).",
}
```

`provision_all()` signature (what a config-driven provisioner must replace):

```python
def provision_all(
    self,
    model: str,                    # "gpt-4.1"
    prompts: dict[str, str],       # {agent_key: prompt_content}
    graph_query_api_uri: str,      # base URL for OpenAPI tools
    graph_backend: str,            # "cosmosdb" or "mock"
    graph_name: str,               # "telco-noc-topology" (X-Graph value)
    runbooks_index: str,           # AI Search index name
    tickets_index: str,            # AI Search index name
    search_connection_id: str,     # Foundry connection ID for AI Search
    force: bool = True,
    on_progress: callable | None = None,
) -> dict:
```

**Exact tool types available in Foundry SDK:**
- `OpenApiTool(name, spec, description, auth)` â€” needs OpenAPI YAML spec dict
- `AzureAISearchTool(index_connection_id, index_name, query_type, top_k)` â€” needs Foundry connection ID
- `ConnectedAgentTool(id, name, description)` â€” needs agent IDs from sub-agents

**V9 plan:** Config declares agents as list of dicts. Each agent declares its tools by type. The provisioner iterates the list, creates matching SDK tool objects, then wires orchestrator â†’ sub-agents via ConnectedAgentTool.

### 5. OpenAPI Spec â€” Exact Template Mechanism

From `graph-query-api/openapi/cosmosdb.yaml`:

```yaml
servers:
  - url: "{base_url}"                   # replaced at runtime
paths:
  /query/graph:
    post:
      parameters:
        - name: X-Graph
          in: header
          schema:
            enum: ["{graph_name}"]      # replaced at runtime
      requestBody:
        schema:
          properties:
            query:
              type: string
              description: |
                A Gremlin traversal query string...
```

**Key details for V9:**
- `{base_url}` replacement: `raw.replace("{base_url}", graph_query_api_uri.rstrip("/"))`
- `{graph_name}` replacement: `raw.replace("{graph_name}", graph_name)`
- `_load_openapi_spec()` has a `keep_path` parameter for prefix-filtering paths
- The **description fields** are Gremlin-specific (mentions `addV`, `has()`, vertex labels)
- The telemetry spec hardcodes `container_name` enum to `["AlertStream", "LinkTelemetry"]`

**V9 gap:** For a new backend (Fabric/KQL), need entirely different:
- Query language description text
- Container/table name enums
- Query syntax examples
These must come from config, not static YAML. Options: (a) Jinja2-templated YAML, (b) programmatic spec generation, (c) per-connector spec templates with config-driven variable injection.

### 6. Telemetry Router â€” Exact Cosmos SQL Coupling

From `graph-query-api/router_telemetry.py`:

```python
container_name = f"{ctx.telemetry_container_prefix}-{req.container_name}"
# Then directly calls:
client = get_cosmos_client()
database = client.get_database_client(db_name)
container = database.get_container_client(container_name)
items = list(container.query_items(query=query, enable_cross_partition_query=True))
```

**V9 gap:** This is a completely separate code path from `GraphBackend`. No adapter abstraction exists for telemetry queries. A `TelemetryBackend` protocol is needed, mirroring `GraphBackend`:

```python
class TelemetryBackend(Protocol):
    async def execute_query(self, query: str, container_name: str, **kwargs) -> dict:
        """Returns {columns: [...], rows: [...]}"""
        ...
```

### 7. Ingest Router â€” Exact Backend-Specific Code

`router_ingest.py` (872 lines) has 5 distinct upload paths, each tightly coupled:

| Upload | Backend Coupling |
|--------|-----------------|
| Graph | `_gremlin_client()`, `_gremlin_submit()`, `_ensure_gremlin_graph()` â€” all Gremlin-specific |
| Telemetry | `get_cosmos_client()`, `container.upsert_item()` â€” Cosmos NoSQL-specific |
| Runbooks | `BlobServiceClient` + `search_indexer.create_search_index()` â€” Blob+Search-specific |
| Tickets | Same as Runbooks |
| Prompts | `_get_prompts_container()`, `container.upsert_item()` â€” Cosmos NoSQL-specific |

**V9 gap:** Ingestion needs adapter abstraction. Each connector must provide:
- `ingest_graph(schema, data_dir, progress)` â€” knows how to load graph data
- `ingest_telemetry(containers_config, data_dir, progress)` â€” knows how to load telemetry
- Infrastructure setup (ARM calls, Gremlin graph creation, etc.) is also backend-specific

The knowledge file uploads (runbooks/tickets) are actually backend-agnostic already â€” they go to Blob + AI Search regardless of graph backend. These can stay as-is.

### 8. Prompt Composition â€” Exact Hardcoded Patterns

From `router_ingest.py`:

```python
PROMPT_AGENT_MAP = {
    "foundry_orchestrator_agent.md": "orchestrator",
    "orchestrator.md": "orchestrator",
    "foundry_telemetry_agent_v2.md": "telemetry",
    "telemetry_agent.md": "telemetry",
    "foundry_runbook_kb_agent.md": "runbook",
    "runbook_agent.md": "runbook",
    "foundry_historical_ticket_agent.md": "ticket",
    "ticket_agent.md": "ticket",
    "alert_storm.md": "default_alert",
    "default_alert.md": "default_alert",
}
```

GraphExplorer is composed from 3 hardcoded files:
```
graph_explorer/core_instructions.md
graph_explorer/core_schema.md
graph_explorer/language_gremlin.md
```
Joined with `\n\n---\n\n`.

**Actual prompt files in telco-noc:**
```
data/prompts/
â”œâ”€â”€ alert_storm.md
â”œâ”€â”€ foundry_historical_ticket_agent.md
â”œâ”€â”€ foundry_orchestrator_agent.md
â”œâ”€â”€ foundry_runbook_kb_agent.md
â”œâ”€â”€ foundry_telemetry_agent_v2.md
â””â”€â”€ graph_explorer/
    â”œâ”€â”€ core_instructions.md
    â”œâ”€â”€ core_schema.md
    â”œâ”€â”€ description.md         # NOT used in composition
    â”œâ”€â”€ language_gremlin.md
    â””â”€â”€ language_mock.md       # NOT used in composition
```

**V9 gap:** The prompt â†” agent mapping must come from config, not a hardcoded dict. Config should declare:
- Per agent: which prompt file(s) to use
- Whether prompt is composed from fragments or single file
- Which language file to use based on connector type (e.g., `language_gremlin.md` vs `language_kql.md`)
- Placeholder substitution patterns (`{graph_name}`, `{scenario_prefix}`)

The existing `description.md` and `language_mock.md` in graph_explorer/ are already evidence of a composition pattern that could be config-driven.

### 9. Config Apply Endpoint â€” Exact Hardcoded Defaults

From `api/app/routers/config.py`:

```python
defaults = {
    "orchestrator": "You are an investigation orchestrator.",
    "graph_explorer": "You are a graph explorer agent.",
    "telemetry": "You are a telemetry analysis agent.",
    "runbook": "You are a runbook knowledge base agent.",
    "ticket": "You are a historical ticket search agent.",
}
```

The endpoint hardcodes:
- Prompt fetching from `http://127.0.0.1:8100/query/prompts?scenario={prefix}`
- Placeholder substitution: `{graph_name}` and `{scenario_prefix}`
- Search connection ID path: `aisearch-connection` (hardcoded name)
- `"All 5 agents re-provisioned"` log message

**V9 plan:** This endpoint reads agent definitions from config instead of hardcoding 5 agents.

### 10. Frontend ScenarioContext â€” Exact Derivation Pattern

From `frontend/src/context/ScenarioContext.tsx`:

```typescript
const deriveGraph = (name: string | null) => name ? `${name}-topology` : 'topology';
const deriveRunbooks = (name: string | null) => name ? `${name}-runbooks-index` : 'runbooks-index';
const deriveTickets = (name: string | null) => name ? `${name}-tickets-index` : 'tickets-index';
const derivePrompts = (name: string | null) => name ?? '';
```

And on `setActiveScenario`:
```typescript
setActiveGraph(`${name}-topology`);
setActiveRunbooksIndex(`${name}-runbooks-index`);
setActiveTicketsIndex(`${name}-tickets-index`);
setActivePromptSet(name);
```

**V9 gap:** These derivation functions assume exactly 3 resource types (graph, runbooks, tickets). It also assumes naming conventions (`-topology`, `-runbooks-index`, `-tickets-index`). With config-driven scenarios, the `SavedScenario` type already has a `resources` field that stores exact names:

```typescript
resources: {
    graph: string;
    telemetry_database: string;
    telemetry_container_prefix?: string;
    runbooks_index: string;
    tickets_index: string;
    prompts_database: string;
    prompts_container?: string;
};
```

**V9 plan:** Instead of deriving from conventions, load `resources` from the saved scenario record and use exact names. The derivation functions become fallbacks for scenarios created before V9.

### 11. Frontend graphConstants.ts â€” Exact Telco-Specific Values

```typescript
export const NODE_COLORS: Record<string, string> = {
  CoreRouter: '#38BDF8', AggSwitch: '#FB923C', BaseStation: '#A78BFA',
  TransportLink: '#3B82F6', MPLSPath: '#C084FC', Service: '#CA8A04',
  SLAPolicy: '#FB7185', BGPSession: '#F472B6',
};
export const NODE_SIZES: Record<string, number> = {
  CoreRouter: 10, AggSwitch: 7, BaseStation: 5, TransportLink: 7,
  MPLSPath: 6, Service: 8, SLAPolicy: 6, BGPSession: 5,
};
```

These are used as last-resort fallback in `useNodeColor.ts` (resolution chain: user override â†’ scenario styles â†’ hardcoded â†’ auto-hash). The auto-hash already handles unknown labels.

**V9 plan:** Keep the fallback chain but empty the hardcoded maps. Scenario-driven styles (`graph_styles` in `scenario.yaml` / `SavedScenario`) already propagate through `ScenarioContext.scenarioNodeColors`. The hardcoded values only matter for backward compatibility with the telco-noc scenario when no `graph_styles` are loaded.

### 12. Stub Agents â€” Exact Hardcoded Names

From `api/app/routers/alert.py`:

```python
agents = ["TelemetryAgent", "GraphExplorerAgent", "RunbookKBAgent", "HistoricalTicketAgent"]
```

And from `api/app/routers/agents.py`, agent listing returns these stubs when no `agent_ids.json` exists.

**V9 plan:** Stub mode should either read agent names from config, or return a generic "not provisioned" response without scenario-specific names.

### 13. scenario.yaml cosmos section â€” Exact Structure

```yaml
cosmos:
  gremlin:
    database: networkgraph
    graph: topology
  nosql:
    database: telemetry
    containers:
      - name: AlertStream
        partition_key: /SourceNodeType
        csv_file: AlertStream.csv
        id_field: AlertId
        numeric_fields: [OpticalPowerDbm, BitErrorRate, CPUUtilPct, PacketLossPct]
      - name: LinkTelemetry
        partition_key: /LinkId
        csv_file: LinkTelemetry.csv
        id_field: null
        numeric_fields: [UtilizationPct, OpticalPowerDbm, BitErrorRate, LatencyMs]
```

**V9 plan:** Replace `cosmos:` section with a `data_stores:` section that supports multiple backends:

```yaml
data_stores:
  graph:
    connector: cosmosdb-gremlin
    config:
      database: networkgraph
      graph: topology
  telemetry:
    connector: cosmosdb-nosql
    config:
      database: telemetry
      containers: [...]
```

### 14. Shared Databases â€” Bicep-Created Pre-Existing Resources

Key architectural constraint from ARCHITECTURE.md: some databases are **shared** and pre-created by Bicep:
- `networkgraph` â€” shared Gremlin database (all scenarios share this)
- `telemetry` â€” shared NoSQL database (per-scenario containers within it)
- `prompts` â€” shared NoSQL database (per-scenario containers within it)
- `scenarios` â€” shared NoSQL database for scenario metadata
- `interactions` â€” shared NoSQL database for interaction history

**V9 implication:** The adapter abstraction must respect this pattern. ARM calls only create containers/graphs within shared databases â€” they don't create databases. If a new backend (Fabric) has different resource creation patterns, the adapter must handle that transparently.

### 15. Search Indexer â€” Exact Pipeline Structure

From `graph-query-api/search_indexer.py`:

Creates: `data source â†’ index (with vector field + HNSW) â†’ skillset (chunk + embed) â†’ indexer`

Uses:
- `AzureOpenAIEmbeddingSkill` â€” requires `AI_FOUNDRY_NAME` for vectorizer endpoint
- `SearchIndexerIndexProjection` â€” chunk projection
- Polls indexer status until complete

**V9 implication:** The search pipeline is backend-agnostic (it operates on Blob â†’ AI Search regardless of graph backend). No adapter changes needed for knowledge files. However, the AI Search connection name (`aisearch-connection`) used by agent tools is hardcoded in the provisioner and must come from config.

### 16. Environment Variables â€” Complete List for V9 Config

Variables currently scattered across config files that a V9 config should consolidate:

| Variable | Used By | Current Default |
|----------|---------|-----------------|
| `GRAPH_BACKEND` | config.py | `"cosmosdb"` |
| `COSMOS_GREMLIN_ENDPOINT` | config.py, router_ingest.py | `""` |
| `COSMOS_GREMLIN_PRIMARY_KEY` | config.py, router_ingest.py | `""` |
| `COSMOS_GREMLIN_DATABASE` | config.py, router_ingest.py | `"networkgraph"` |
| `COSMOS_GREMLIN_GRAPH` | config.py | `"topology"` |
| `COSMOS_NOSQL_ENDPOINT` | config.py, router_telemetry.py, router_prompts.py | `""` |
| `COSMOS_NOSQL_DATABASE` | config.py | `"telemetry"` |
| `AI_SEARCH_NAME` | config.py, router_ingest.py | `""` |
| `STORAGE_ACCOUNT_NAME` | router_ingest.py | `""` |
| `PROJECT_ENDPOINT` | api config.py | `""` |
| `AI_FOUNDRY_PROJECT_NAME` | api config.py | `""` |
| `AI_FOUNDRY_NAME` | api config.py, search_indexer.py | `""` |
| `MODEL_DEPLOYMENT_NAME` | api config.py | `"gpt-4.1"` |
| `GRAPH_QUERY_API_URI` | api config.py | `""` |
| `CONTAINER_APP_HOSTNAME` | api config.py | `""` |
| `AZURE_SUBSCRIPTION_ID` | router_ingest.py, api config.py | `""` |
| `AZURE_RESOURCE_GROUP` | router_ingest.py, api config.py | `""` |
| `DEFAULT_SCENARIO` | azure_config.env.template | `"telco-noc"` |

**V9 plan:** Env vars remain for cloud/infra settings. Per-scenario settings move to config YAML. Backend connection details can either stay as env vars (shared across scenarios) or be per-connector in config.

### 17. Open Questions for Implementation

1. **Where does the V9 config file live at runtime?** Options:
   - In the `scenarios/` database in Cosmos (alongside scenario metadata)
   - As a YAML file in Blob Storage
   - Embedded as an extended `scenario.yaml` within each data pack
   - The existing `scenario.yaml` already covers data layout + graph styles + cosmos config â€” is extending it sufficient, or do we need a separate platform-level config?

2. **Adapter registration mechanism**: pip-installable plugins? Simple Python module discovery? A `connectors/` directory with `__init__.py` that auto-registers?

3. **Should telemetry and graph share one adapter, or be separate?** Currently they use different databases and different query languages (Gremlin vs Cosmos SQL). A single "cosmosdb" adapter handling both, or separate `cosmosdb-gremlin` + `cosmosdb-nosql` adapters?

4. **OpenAPI spec generation vs templating**: Generate specs programmatically from adapter metadata (query language, available containers/tables, field descriptions), or use Jinja2-templated YAML files per connector?

5. **Migration strategy for telco-noc prompts**: The `graph_explorer/` composition pattern works well. Should all agents support composition from fragments, or only agents that need connector-specific language sections?

6. **Prompt placeholder expansion**: Currently `{graph_name}` and `{scenario_prefix}`. Need to define the full set of placeholders that config-driven prompts can use, and when expansion happens (upload time vs provisioning time vs both, as currently).