# v11c — Data Retrieval Performance Optimisation

**Status:** Ready for implementation
**Author:** Copilot
**Date:** 2026-02-16
**Depends on:** v11b (streaming fixes)
**Audited:** 2026-02-16 (18 issues found & incorporated)

---

## Problem Statement

Initial page load and scenario selection feel sluggish. Two core endpoints are
the bottleneck:

| Endpoint | Measured TTFB | Server query time | Overhead | Payload |
|---|---|---|---|---|
| `GET /query/scenarios/saved` | ~630ms | ~300ms | ~330ms (TLS + network) | ~4 KB |
| `POST /query/topology` | ~1120ms | **29ms** | ~1090ms | ~20 KB |

Root causes:

1. **Gremlin WSS connection overhead** — gremlinpython establishes a fresh
   WebSocket on the first query per graph, then `asyncio.to_thread` adds
   thread-pool scheduling latency on every call.
2. **No caching** — topology data changes only on scenario upload/delete, yet
   every page load re-fetches from Cosmos.
3. **Sync Cosmos SDK via `asyncio.to_thread`** — `CosmosDocumentStore.list()`
   wraps sync calls, adding thread pool overhead.
4. **Duplicate frontend fetches** — `ScenarioContext`, `useScenarios` hook,
   `InvestigationPanel`, and `ScenarioInfoPanel` all independently call
   `/query/scenarios/saved` on mount — up to **4 redundant requests**.

---

## Architecture Constraint: Multi-Replica

`infra/main.bicep` sets `minReplicas: 1, maxReplicas: 3` with an HTTP scale
rule. All caches in this plan are **in-memory, per-replica**. After a graph
upload or scenario save, only the handling replica's cache is invalidated. Other
replicas serve stale data until TTL expiry.

**Accepted trade-off:** Topology changes are rare (manual uploads only) and
the uploading user is routed to the same replica (Azure Container Apps sticky
sessions). A short TTL (30s topology, 15s scenarios) bounds staleness.

---

## Plan

### Phase 1a: Topology TTL Cache

**Files:** `graph-query-api/router_topology.py`, `graph-query-api/models.py`,
`graph-query-api/ingest/graph_ingest.py`, `graph-query-api/ingest/scenarios.py`

**Effort:** 30 min | **Impact:** High — 1100ms → <5ms (warm) | **Risk:** Low

#### 1. Add `cached` field to `TopologyMeta`

**File:** `graph-query-api/models.py`

```python
class TopologyMeta(BaseModel):
    node_count: int
    edge_count: int
    query_time_ms: float
    labels: list[str] = []
    cached: bool = False        # ← NEW
```

Without this, cached responses report `query_time_ms: 0` which is misleading.

#### 2. Add TTL cache to `router_topology.py`

```python
import threading, time

_topo_cache: dict[str, tuple[float, float, dict]] = {}
#                  key → (expires_at, orig_query_ms, response_dict)
_topo_lock = threading.Lock()
TOPO_TTL = 30  # seconds


def invalidate_topology_cache(graph_name: str | None = None) -> None:
    """Clear topology cache entries. Called by ingest after graph mutations.

    If graph_name is provided, only entries for that graph are cleared.
    If None, the entire cache is cleared.
    """
    with _topo_lock:
        if graph_name is None:
            _topo_cache.clear()
        else:
            to_delete = [k for k in _topo_cache if k.startswith(f"{graph_name}:")]
            for k in to_delete:
                del _topo_cache[k]
```

Modify the `topology()` endpoint:

```python
async def topology(req, ctx=Depends(get_scenario_context)):
    backend = get_backend_for_context(ctx)

    # Normalise cache key: None and [] both mean "all vertices" → same key
    labels = sorted(req.vertex_labels) if req.vertex_labels else []
    cache_key = f"{ctx.graph_name}:{','.join(labels)}"

    # Check cache
    with _topo_lock:
        hit = _topo_cache.get(cache_key)
        if hit:
            exp, orig_ms, cached_dict = hit
            if time.time() < exp:
                # Return fresh TopologyResponse from cached dict
                meta = TopologyMeta(**{**cached_dict["meta"], "cached": True})
                return TopologyResponse(**{**cached_dict, "meta": meta})

    # Cache miss — run actual query
    t0 = time.perf_counter()
    result = await backend.get_topology(
        query=req.query, vertex_labels=req.vertex_labels,
    )
    elapsed = (time.perf_counter() - t0) * 1000
    nodes = result.get("nodes", [])
    edges = result.get("edges", [])
    response = TopologyResponse(
        nodes=nodes, edges=edges,
        meta=TopologyMeta(
            node_count=len(nodes), edge_count=len(edges),
            query_time_ms=round(elapsed, 1),
            labels=sorted({n["label"] for n in nodes}),
            cached=False,
        ),
    )

    # Cache the DICT, not the Pydantic object — avoids mutable shared state
    with _topo_lock:
        _topo_cache[cache_key] = (
            time.time() + TOPO_TTL,
            round(elapsed, 1),
            response.model_dump(),
        )
    return response
```

**Design decisions:**
- Cache `response.model_dump()` (a plain dict), not the live Pydantic object.
  `TopologyResponse.nodes` is a mutable `list[TopologyNode]` — returning the
  same object to concurrent requests risks shared mutation.
- Reconstruct `TopologyResponse` from cached dict on each hit — safe and cheap.
- Normalise `vertex_labels`: both `None` and `[]` produce key `"graph_name:"`.

#### 3. Cache invalidation (2 mutation points)

The graph upload and scenario delete endpoints are in the same process — use a
direct function import, not an HTTP call.

**File:** `graph-query-api/ingest/graph_ingest.py` — at end of `upload_graph()`:

```python
from router_topology import invalidate_topology_cache
invalidate_topology_cache(graph_name)
```

**File:** `graph-query-api/ingest/scenarios.py` — at end of `delete_scenario()`:

```python
from router_topology import invalidate_topology_cache
invalidate_topology_cache(graph_name)
```

---

### Phase 1b: Scenario List TTL Cache

**Files:** `graph-query-api/router_scenarios.py`

**Effort:** 20 min | **Impact:** High — 630ms → <5ms (warm) | **Risk:** Low

#### 1. Singleton `_get_store()`

`_get_store()` currently creates a new `CosmosDocumentStore` on every call.
While `_container_cache` in `cosmos_helpers.py` prevents repeated ARM calls,
the object allocation is needless. Make it a lazy singleton:

```python
_store: DocumentStore | None = None

def _get_store() -> DocumentStore:
    """Get (or create) the singleton DocumentStore for scenario metadata."""
    global _store
    if _store is None:
        _store = get_document_store(
            SCENARIOS_DATABASE, SCENARIOS_CONTAINER, "/id",
            ensure_created=True,
        )
    return _store
```

> **Cold start note:** The first call triggers a synchronous ARM
> `begin_create_update_sql_container().result()` (~5-15s) on the event loop.
> This is an existing issue, not introduced by caching. The singleton just
> ensures it happens only once. A future improvement could warm this in the
> lifespan handler, but that's out of scope for v11c.

#### 2. Add TTL cache

```python
import threading, time

_scenarios_cache: tuple[float, list[dict]] | None = None
_scenarios_lock = threading.Lock()
SCENARIOS_TTL = 15  # seconds


def invalidate_scenarios_cache() -> None:
    """Clear the cached scenario list."""
    global _scenarios_cache
    with _scenarios_lock:
        _scenarios_cache = None
```

Modify `list_saved_scenarios()`:

```python
async def list_saved_scenarios():
    global _scenarios_cache
    with _scenarios_lock:
        if _scenarios_cache and time.time() < _scenarios_cache[0]:
            return {"scenarios": _scenarios_cache[1]}

    store = _get_store()
    items = await store.list(query="SELECT * FROM c ORDER BY c.updated_at DESC")

    with _scenarios_lock:
        _scenarios_cache = (time.time() + SCENARIOS_TTL, items)
    return {"scenarios": items}
```

#### 3. Invalidation on mutations

Call `invalidate_scenarios_cache()` at the end of both `save_scenario()` and
`delete_saved_scenario()` in the same file (after the upsert/delete succeeds).

---

### Phase 2: Eliminate Duplicate Frontend Fetches

**Files:** `frontend/src/hooks/useScenarios.ts`,
`frontend/src/components/ScenarioChip.tsx`,
`frontend/src/components/InvestigationPanel.tsx`,
`frontend/src/components/ScenarioInfoPanel.tsx`

**Effort:** 1 hr | **Impact:** Medium — 4x → 1x fetch on page load | **Risk:** Low

#### Problem Detail

`ScenarioContext` is the canonical store for `savedScenarios` and fetches on
mount via `refreshScenarios()`. But `useScenarios()` maintains a **second**
independent `savedScenarios` state with its own `fetchSavedScenarios()`. Three
components instantiate `useScenarios()` independently (ScenarioChip,
InvestigationPanel, ScenarioInfoPanel), each getting their own copy. This
causes 2-4 redundant `GET /query/scenarios/saved` calls on every page load.

#### Changes

**1. `useScenarios.ts` — Remove duplicate state, read from context**

Remove:
- `const [savedScenarios, setSavedScenarios] = useState<SavedScenario[]>([])`
- `const [savedLoading, setSavedLoading] = useState(false)`
- The entire `fetchSavedScenarios` callback

Add to the existing context destructure:
```typescript
const {
  setActiveScenario,
  setProvisioningStatus,
  setScenarioStyles,
  savedScenarios,           // ← ADD
  scenariosLoading,         // ← ADD
  refreshScenarios,         // ← ADD
} = useScenarioContext();
```

**2. `saveScenario` — Switch refresh call**

```typescript
const saveScenario = useCallback(async (meta: {...}) => {
  const res = await fetch('/query/scenarios/save', { ... });
  if (!res.ok) { ... }
  const data = await res.json();
  await refreshScenarios();  // was: await fetchSavedScenarios()
  return data;
}, [refreshScenarios]);
```

**3. `deleteSavedScenario` — Same fix**

```typescript
const deleteSavedScenario = useCallback(async (name: string) => {
  const res = await fetch(`/query/scenarios/saved/${...}`, { method: 'DELETE' });
  if (!res.ok) { ... }
  await refreshScenarios();  // was: await fetchSavedScenarios()
}, [refreshScenarios]);
```

> Note: `deleteSavedScenario` is currently **unused** — no component imports
> it. Keep it working but consider removing in a future cleanup.

**4. `selectScenario` — Closure bug fix**

The current code has `const saved = savedScenarios.find(s => s.id === name)`
which previously captured the hook-local `savedScenarios`. After removing
hook-local state, this correctly reads from context's `savedScenarios`. Verify
the dependency array includes `savedScenarios` from context (it already does —
no change needed, but verify after edit).

**5. Return object — Export context-backed values**

```typescript
return {
  scenarios, indexes, loading, error,
  fetchScenarios, fetchIndexes,
  // Saved scenario management — now backed by context
  savedScenarios,                          // from context
  savedLoading: scenariosLoading,          // from context (renamed for compat)
  fetchSavedScenarios: refreshScenarios,   // alias for backward compat
  saveScenario, deleteSavedScenario, selectScenario,
};
```

**6. `ScenarioChip.tsx` — Remove lazy-fetch `useEffect`**

ScenarioChip has a `useEffect` that calls `fetchSavedScenarios()` on first
dropdown open (`dropdownOpen && !hasFetched`). Since context already fetches on
mount, this is redundant. Remove:
- The `hasFetched` ref/state
- The `useEffect` that conditionally fetches on dropdown open

**7. `InvestigationPanel.tsx` / `ScenarioInfoPanel.tsx` — Remove mount-time fetch**

Both have:
```typescript
useEffect(() => { fetchSavedScenarios(); }, [fetchSavedScenarios]);
```

Remove these `useEffect` calls entirely. The data is already in context.

#### Post-refactor verification

- Confirm InvestigationPanel's example question lookup still works:
  `savedScenarios.find(s => s.id === activeScenario)?.example_questions`
  (reads from context now — same data, correct).
- Confirm ScenarioChip dropdown populates immediately (no lazy fetch delay).
- Open Network tab: only **1** call to `/query/scenarios/saved` on page load.

---

### Phase 3: Gremlin Connection Warm-Up

**Files:** `graph-query-api/backends/cosmosdb.py`, `graph-query-api/main.py`

**Effort:** 1 hr | **Impact:** Medium — cold start 1100ms → 400ms | **Risk:** Medium

#### Problem

The biggest topology overhead is the initial Gremlin WSS handshake (~700ms TLS
negotiation). The connection is lazily created on first query.

#### Implementation

**Do NOT call `_get_client()` in `__init__()`** — `__init__` is called from
`get_backend_for_graph()` which holds `_backend_lock`. Holding the lock for
~700ms during WSS handshake blocks ALL other backend lookups.

Instead, pre-warm asynchronously during the lifespan handler.

**1. `graph-query-api/backends/cosmosdb.py` — Add warm-up method**

```python
def warm_connection(self) -> None:
    """Eagerly establish the Gremlin WSS connection.

    Call via asyncio.to_thread() to avoid blocking the event loop.
    """
    try:
        self._get_client()
        logger.info("Gremlin WSS connection warmed for graph=%s", self._graph_name)
    except Exception as e:
        logger.warning("Gremlin warm-up failed for graph=%s: %s", self._graph_name, e)
```

**2. `graph-query-api/backends/cosmosdb.py` — Add keepalive loop**

```python
async def keepalive_loop(self) -> None:
    """Periodic ping to prevent Cosmos Gremlin idle disconnect (30 min)."""
    while True:
        await asyncio.sleep(300)  # 5 minutes
        try:
            await asyncio.to_thread(self._submit_query, "g.V().count()")
        except Exception:
            pass  # will reconnect on next real query
```

> **RU cost:** ~3 RU per ping × 12 pings/hr = ~36 RU/hr per graph. Negligible.

**3. `graph-query-api/main.py` — Pre-warm in lifespan**

```python
@asynccontextmanager
async def _lifespan(app: FastAPI):
    # ... existing config validation ...

    # Pre-warm Gremlin backends for known scenarios
    keepalive_tasks: list[asyncio.Task] = []
    try:
        from router_scenarios import _get_store
        store = _get_store()
        scenarios = await store.list(query="SELECT c.id, c.resources FROM c")
        for s in scenarios:
            graph = s.get("resources", {}).get("graph")
            if graph:
                try:
                    backend = get_backend_for_graph(graph)
                    if hasattr(backend, "warm_connection"):
                        await asyncio.to_thread(backend.warm_connection)
                    if hasattr(backend, "keepalive_loop"):
                        task = asyncio.create_task(backend.keepalive_loop())
                        keepalive_tasks.append(task)
                except Exception as e:
                    logger.warning("Pre-warm failed for graph=%s: %s", graph, e)
    except Exception as e:
        logger.warning("Skipping Gremlin pre-warm (scenarios unavailable): %s", e)

    logger.info("Starting with GRAPH_BACKEND=%s", GRAPH_BACKEND)
    yield

    # Cancel keepalive tasks on shutdown
    for task in keepalive_tasks:
        task.cancel()

    await close_graph_backend()
    close_telemetry_backend()
```

> **Fresh deployment safety:** If the scenarios database doesn't exist yet,
> the `try/except` catches the error and logs a warning. The app starts
> normally — backends warm lazily on first request.

---

### Deferred Phases

**Phase 4: Combined Gremlin V()/E() query** — Cosmos Gremlin has limited
`union()` support, and the gain (~29ms → ~20ms) is marginal compared to Phase
1a caching. Not worth the compatibility risk.

**Phase 5: Async Cosmos NoSQL SDK** — Phase 1b caching makes this irrelevant
for scenarios. Revisit only if high-concurrency becomes a concern.

---

## Implementation Priority

| Phase | Effort | Impact | Risk |
|---|---|---|---|
| 1a. Topology TTL cache | Small (30 min) | **High** — 1100ms → <5ms (warm) | Low |
| 1b. Scenario list TTL cache | Small (20 min) | **High** — 630ms → <5ms (warm) | Low |
| 2. Deduplicate frontend fetches | Medium (1 hr) | **Medium** — 4x → 1x calls | Low |
| 3. Gremlin connection warm-up | Medium (1 hr) | **Medium** — cold start 1100ms → 400ms | Medium |

**Execution order:** 1a → 1b → 2 → 3 → deploy → measure.

---

## Files Modified (Summary)

| File | Phase | Changes |
|---|---|---|
| `graph-query-api/models.py` | 1a | Add `cached: bool = False` to `TopologyMeta` |
| `graph-query-api/router_topology.py` | 1a | Add TTL cache + `invalidate_topology_cache()` |
| `graph-query-api/ingest/graph_ingest.py` | 1a | Call `invalidate_topology_cache()` after upload |
| `graph-query-api/ingest/scenarios.py` | 1a | Call `invalidate_topology_cache()` after delete |
| `graph-query-api/router_scenarios.py` | 1b | Add TTL cache + singleton `_get_store()` + `invalidate_scenarios_cache()` |
| `frontend/src/hooks/useScenarios.ts` | 2 | Remove duplicate state, read from context |
| `frontend/src/components/ScenarioChip.tsx` | 2 | Remove lazy-fetch useEffect |
| `frontend/src/components/InvestigationPanel.tsx` | 2 | Remove mount-time fetchSavedScenarios |
| `frontend/src/components/ScenarioInfoPanel.tsx` | 2 | Remove mount-time fetchSavedScenarios |
| `graph-query-api/backends/cosmosdb.py` | 3 | Add `warm_connection()` + `keepalive_loop()` |
| `graph-query-api/main.py` | 3 | Pre-warm backends + keepalive tasks in lifespan |

---

## Measurements (Baseline — 2026-02-16)

All from WSL → Sweden Central Container App over public internet.

```
Endpoint                    TTFB     Server Query   Payload   Notes
─────────────────────────── ──────── ────────────── ───────── ─────────────────────
GET  /query/scenarios/saved   633ms   ~300ms (est.)    4 KB   Cross-partition NoSQL
POST /query/topology         1120ms       29ms        20 KB   2x Gremlin WSS queries
GET  /api/logs (SSE)           —          —            SSE    Working (v11b fix)
```

Graph: 50 nodes, 66 edges (telco-noc scenario).

---

## Success Criteria

After Phase 1+2:
- [ ] Warm scenario list: < 50ms TTFB
- [ ] Warm topology load: < 50ms TTFB
- [ ] Only 1 `/query/scenarios/saved` call on page load (down from 2-4)
- [ ] Cache invalidated on scenario upload/delete (no stale data)
- [ ] `cached: true` in topology meta on cache hit

After Phase 3:
- [ ] Cold topology load (first after deploy): < 500ms TTFB
Summary of fixes deployed
Three problems were visible in the logs. All three are now fixed and deployed.

1. 31-second startup block — ARM partition key conflict
Root cause: On every container startup, _arm_ensure_container() called begin_create_update_sql_container() for the scenarios/configs container with partition key /scenario_name. But the container already exists in Azure with partition key /id. Since Cosmos DB partition keys are immutable, the ARM PUT returned a BadRequest error — and the .result() call blocked for ~31 seconds waiting for the long-running operation to fail.

This blocked all incoming requests during startup (the scenarios, topology, logs, and interactions calls you see at 09:32:21 all waited ~31s before getting responses at 09:32:52).

Fix (2 files):

config_store.py — Changed partition key from /scenario_name to /id to match the existing container (safe because every document already sets "id": scenario_name)
cosmos_helpers.py — _arm_ensure_container() now calls get_sql_container() first to check if the container exists. If it does, it skips the ARM PUT entirely. This prevents the error for all 5 callers that use ensure_created=True, not just configs.
2. 6+ duplicate POST /query/topology calls
Root cause: Two frontend issues causing re-render storms:

The ScenarioContext provider created a new object reference on every render, forcing all consumers to re-render even when values hadn't changed
useTopology's useEffect had no cleanup function to abort in-flight requests, so React StrictMode's double-invoke and re-render cascades each fired a real network call
Fix (2 files):

ScenarioContext.tsx — Wrapped the provider value in useMemo with all dependencies, so consumers only re-render when an actual value changes
useTopology.ts — Added return () => { abortRef.current?.abort(); } cleanup to the effect, so superseded fetches are cancelled before they hit the network
3. Cache verification
Confirmed the TTL caches from v11c are working correctly:

Request	cached	query_time_ms	TTFB
Cold topology	false	129.9ms (live query)	1.75s
Warm topology	true	129.9ms (stored)	0.81s
Warm scenarios	—	—	0.63s
The ~600-800ms warm TTFB is pure TLS handshake + network round-trip from WSL to Sweden Central — the server-side response is effectively instant on cache hits. The scenarios/saved second call in the logs already showed 2ms server-side.