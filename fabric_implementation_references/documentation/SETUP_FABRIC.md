# Fabric Backend Setup

Setup guide for `GRAPH_BACKEND=fabric` — the production-scale path using
Microsoft Fabric GraphModel with GQL queries over Lakehouse-backed ontology.

> **Note:** Fabric Ontology (GraphModel) is a **preview feature** and may
> exhibit unexpected behaviors. Graph indexing after ontology creation takes
> 20–90 minutes.

---

## Prerequisites

- Completed [main Prerequisites](../README.md#prerequisites) (tools, auth)
- `GRAPH_BACKEND=fabric` set in `azure_config.env`
- `AZURE_FABRIC_ADMIN` set to the email of the Fabric capacity administrator
- A Microsoft Fabric license or trial (the F-SKU capacity is provisioned by Bicep)

---

## Deployment

### 1. Configure

```bash
cp azure_config.env.template azure_config.env
```

Edit `azure_config.env`:
```bash
GRAPH_BACKEND=fabric
AZURE_FABRIC_ADMIN=your-email@company.com
FABRIC_SKU=F8                    # F2–F2048, pause when not in use
FABRIC_WORKSPACE_NAME=AutonomousNetworkDemo
FABRIC_LAKEHOUSE_NAME=NetworkTopologyLH
FABRIC_EVENTHOUSE_NAME=NetworkTelemetryEH_3117
```

### 2. Deploy infrastructure

```bash
azd up -e <env-name>
```

This provisions:
- Fabric capacity (F-SKU) — **incurs charges while running** (~$1.05/hr for F8)
- All shared resources (AI Foundry, Search, Storage, Container Apps)
- `graph-query-api` Container App with `GRAPH_BACKEND=fabric`

### 3. Provision Fabric workspace & data stores

These scripts must be run in order — each depends on the previous:

```bash
# 3a. Create Lakehouse and upload topology CSVs
uv run python scripts/fabric/provision_lakehouse.py

# 3b. Create Eventhouse and populate telemetry tables
uv run python scripts/fabric/provision_eventhouse.py

# 3c. Create ontology (GraphModel) over Lakehouse tables
# This takes 20-90 minutes for graph indexing to complete
uv run python scripts/fabric/provision_ontology.py
```

### 4. Discover Fabric resource IDs

After creating all Fabric resources, discover their GUIDs and write them to
`azure_config.env`:

```bash
uv run python scripts/fabric/populate_fabric_config.py
```

This populates:
- `FABRIC_CAPACITY_ID`
- `FABRIC_WORKSPACE_ID`
- `FABRIC_LAKEHOUSE_ID`
- `FABRIC_EVENTHOUSE_ID`
- `FABRIC_KQL_DB_ID`, `FABRIC_KQL_DB_NAME`
- `EVENTHOUSE_QUERY_URI`

### 5. Grant Container App access to Fabric

The `graph-query-api` Container App uses its system-assigned managed identity to
call the Fabric GQL/KQL APIs. It must be a workspace member:

```bash
uv run python scripts/fabric/assign_fabric_role.py
```

This reads `FABRIC_WORKSPACE_ID` and `GRAPH_QUERY_API_PRINCIPAL_ID` from
`azure_config.env` and assigns the Contributor role via the Fabric REST API.
Re-running is safe — it skips if the role already exists.

### 6. Update Container App env vars

Push the Fabric IDs to the deployed Container App:

```bash
source azure_config.env
CA_NAME=<container-app-name>   # from Azure portal
RG=$AZURE_RESOURCE_GROUP

az containerapp update --name $CA_NAME --resource-group $RG \
  --set-env-vars \
    "FABRIC_WORKSPACE_ID=$FABRIC_WORKSPACE_ID" \
    "FABRIC_GRAPH_MODEL_ID=$FABRIC_GRAPH_MODEL_ID" \
    "EVENTHOUSE_QUERY_URI=$EVENTHOUSE_QUERY_URI" \
    "FABRIC_KQL_DB_NAME=$FABRIC_KQL_DB_NAME"
```

Or simply redeploy:

```bash
azd deploy graph-query-api
```

### 7. Verify

```bash
uv run python scripts/testing_scripts/test_graph_query_api.py
```

---

## How It Works

The Fabric backend (`graph-query-api/backends/fabric.py`) uses:

- **Fabric REST API** for GQL queries (`POST /GraphModels/{id}/executeQuery`)
- **DefaultAzureCredential** for managed identity auth (non-blocking token acquisition via `asyncio.to_thread`)
- **Persistent httpx.AsyncClient** for connection pooling
- **Retry with backoff** on HTTP 429 (rate limiting), with Retry-After header parsing

GQL queries generated by the GraphExplorer agent are sent to the Fabric GraphModel
API. The ontology defines the graph schema over Lakehouse tables — no data
duplication, the graph is a view over the CSVs.

### Ontology property naming

GQL queries must use **ontology property names**, not Lakehouse column names.
These are defined in `scripts/fabric/provision_ontology.py`. When property names
match CSV column names (the recommended convention), this isn't an issue.

When adding new entity types:
1. Add the entity and properties in `provision_ontology.py`
2. Update the agent prompt in `data/prompts/graph_explorer/core_schema.md`
3. Test the GQL query directly via `/query/graph` — Fabric 400 errors include
   available property names in the error message

---

## Fabric Scripts Reference

| Script | Purpose |
|--------|---------|
| `scripts/fabric/provision_lakehouse.py` | Create Lakehouse, upload CSVs, create shortcuts |
| `scripts/fabric/provision_eventhouse.py` | Create Eventhouse, KQL database, populate tables |
| `scripts/fabric/provision_ontology.py` | Create GraphModel ontology over Lakehouse |
| `scripts/fabric/populate_fabric_config.py` | Discover Fabric resource IDs → `azure_config.env` |
| `scripts/fabric/assign_fabric_role.py` | Grant Container App managed identity workspace access |
| `scripts/fabric/collect_fabric_agents.py` | Discover Fabric Data Agent IDs (if using Data Agent path) |

---

## Cost

- **Fabric capacity (F8):** ~$1.05/hr while running — **pause when not in use**
- **Container App:** 0.25 vCPU, 0.5 GiB (~$0.012/hr)
- **Graph indexing:** no additional cost (included in capacity)

### Pausing Fabric capacity

```bash
# Pause (stops billing)
az fabric capacity suspend --capacity-name <name> --resource-group $AZURE_RESOURCE_GROUP

# Resume
az fabric capacity resume --capacity-name <name> --resource-group $AZURE_RESOURCE_GROUP
```

While paused, all Fabric queries (graph + telemetry) will fail.

---

## Troubleshooting

### Ontology indexing not completing

- Graph indexing takes 20–90 minutes after `provision_ontology.py`
- Sweden Central may be faster due to less regional contention
- Check status in the Fabric portal under the GraphModel item

### GQL 400 errors

Common causes:
- **Property not found** — property name doesn't match ontology definition
- **Relationship direction wrong** — edge `->` vs `<-` doesn't match source/target
- **Entity type not found** — typo in node label (e.g. `SlaPolicy` vs `SLAPolicy`)

Enable debug mode for raw Fabric responses:
```bash
az containerapp update --name $CA_NAME --resource-group $RG \
  --set-env-vars "DEBUG_ENDPOINTS=1"
```
Then query `/debug/raw-gql` for unprocessed API responses.

### 429 Rate limiting

Fabric has per-capacity rate limits. The backend retries automatically with
exponential backoff. If persistent:
- Scale up the Fabric SKU (F8 → F16)
- Reduce query frequency
- Check Fabric capacity utilization in the Azure portal

### "FABRIC_WORKSPACE_ID not set"

Run `uv run python scripts/fabric/populate_fabric_config.py` to auto-discover
all Fabric resource IDs. Then redeploy or update Container App env vars.
