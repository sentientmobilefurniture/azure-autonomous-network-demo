# V11E2 — Remaining Work from V11E

> **Status:** Not started  
> **Depends on:** V11E (Tasks 1–5, 8–11 complete)  
> **Effort:** ~3 hours

---

## Audit Summary

V11E defined 12 implementation tasks. **8 are done, 1 is partial, 3 are not done.**

| # | Task | Status | Notes |
|---|------|--------|-------|
| 1 | Connection CRUD endpoints | ✅ Done | `graph-query-api/router_fabric_connections.py` |
| 2 | Register connection router | ✅ Done | `graph-query-api/main.py` L44, L169 |
| 3 | FabricConnectionPanel | ✅ Done | `frontend/src/components/FabricConnectionPanel.tsx` |
| 4 | Delete FabricSetupWizard | ✅ Done | File removed |
| 5 | Fix graph slot for Fabric | ✅ Done | `AddScenarioModal.tsx` shows `FileSlot` with "Graph Data (Fabric)" label |
| **6** | **Graph tarball → Fabric endpoint** | **❌ Not done** | No `POST /api/fabric/provision/graph` endpoint |
| **7** | **Route graph upload by backend** | **❌ Not done** | `useScenarioUpload.ts` has no Fabric-aware routing |
| **8** | **Upload jobs — Cosmos persistence** | **⚠️ Partial** | Endpoints work but use in-memory `_jobs` dict, not Cosmos |
| 9 | Register upload jobs router | ✅ Done | `api/app/main.py` L28, L55 |
| 10 | Scenario Status Panel | ✅ Done | `frontend/src/components/ScenarioStatusPanel.tsx` |
| 11 | Header changes | ✅ Done | `Header.tsx` has Fabric + status badge buttons |
| **12** | **Modal → background job** | **❌ Not done** | `handleSave` still runs sequential `uploadWithSSE` per slot |

---

## Architecture Context

Two backend services run behind nginx:

| Service | Port | Prefix | Has Cosmos SDK? |
|---------|------|--------|-----------------|
| **api** (FastAPI) | 8000 | `/api/*` | No — uses `httpx` to proxy to graph-query-api |
| **graph-query-api** (FastAPI) | 8100 | `/query/*` | Yes — `stores/` module with `DocumentStore` protocol |

`upload_jobs.py` lives in the **api** service. It uses `httpx` to call graph-query-api endpoints for the actual file ingestion. For Cosmos persistence of job status, it must also proxy through graph-query-api (it cannot import `stores` directly).

`fabric_provision.py` also lives in the **api** service. The existing `/provision` endpoint reads data from the local filesystem via `_resolve_scenario_data()` — no endpoint currently accepts a user-uploaded tarball.

---

## 1. Graph Tarball → Fabric Pipeline Endpoint (V11E Task 6)

### Problem

`api/app/routers/fabric_provision.py` has endpoints for full provisioning (`POST /provision`) and individual steps (`/provision/lakehouse`, `/provision/eventhouse`, `/provision/ontology`), but none accept a user-uploaded graph tarball. The existing `/provision` reads CSVs from the local `data/scenarios/` directory via `_resolve_scenario_data()`.

The required helper functions all exist:
- `_upload_csvs_to_onelake(workspace_name, lakehouse_name, entities_dir, on_progress)` — takes **names** not IDs, uses ADLS Gen2
- `_load_delta_tables(client, workspace_id, lakehouse_id, table_names, on_progress)` — takes an `AsyncFabricClient` + IDs
- `_build_ontology_definition(workspace_id, lakehouse_id, ontology_name)` — returns parts array
- `_apply_ontology_definition(client, workspace_id, ontology_id, parts)` — applies via Fabric API
- `_discover_graph_model(client, workspace_id, ontology_name)` — finds auto-created Graph Model

### What To Build

**File:** `api/app/routers/fabric_provision.py`  
**Endpoint:** `POST /api/fabric/provision/graph`

```python
@router.post("/provision/graph")
async def provision_graph_from_tarball(
    file: UploadFile = File(...),
    workspace_id: str = Form(...),
    workspace_name: str = Form(...),
    lakehouse_name: str = Form("lakehouse"),
    ontology_name: str = Form("ontology"),
):
```

Steps (mirroring the existing `/provision` endpoint's graph_connector branch):

1. Save uploaded tarball to temp directory, extract with `tarfile`
2. Find the `entities/` subdirectory containing CSVs (same layout as `data/scenarios/*/entities/`)
3. `_find_or_create_lakehouse(client, workspace_id, lakehouse_name)` → `lakehouse_id`
4. `_upload_csvs_to_onelake(workspace_name, lakehouse_name, entities_dir)` — note: takes **names** not IDs
5. `_load_delta_tables(client, workspace_id, lakehouse_id, uploaded_table_names)`
6. `_find_or_create_ontology(client, workspace_id, ontology_name)` → `ontology_id`
7. `_build_ontology_definition(workspace_id, lakehouse_id, ontology_name)` → `parts`
8. `_apply_ontology_definition(client, workspace_id, ontology_id, parts)`
9. `_discover_graph_model(client, workspace_id, ontology_name)` → `graph_model_id`

Return: SSE stream via `EventSourceResponse` using existing `_sse_event()` helper and `sse_provision_stream()` wrapper. Progress events match the format the frontend already parses.

**Import needed:** `from fastapi import UploadFile, File, Form` (already partially imported)

**Concurrency:** Use the existing `_fabric_provision_lock` to prevent concurrent provisioning.

---

## 2. Route Graph Upload to Fabric Endpoint (V11E Task 7)

### Problem

`useScenarioUpload.ts` determines the upload endpoint from static `SLOT_DEFS` — graph always goes to `/query/upload/graph`. The `selectedBackend` state exists in `AddScenarioModal.tsx` but is never passed to the hook.

### What To Change

**File:** `frontend/src/hooks/useScenarioUpload.ts`

1. **Add to `UseScenarioUploadProps`:**
   ```typescript
   selectedBackend?: 'cosmosdb-gremlin' | 'fabric-gql';
   ```

2. **In `startUpload()`, resolve endpoint dynamically:**
   ```typescript
   // Inside the SLOT_DEFS loop
   let endpoint = def.endpoint;
   if (def.key === 'graph' && selectedBackend === 'fabric-gql') {
     endpoint = '/api/fabric/provision/graph';
   }
   ```

3. **Pass extra form params for Fabric graph upload:**
   The `uploadWithSSE` call already accepts an `extraParams` object (4th argument, currently `{ scenario_name: name }`). Add `workspace_id` and `workspace_name` when targeting Fabric. These values come from the active Fabric connection — fetch from `/query/fabric/connections` and find `active: true`, or read from the `__active_fabric_config__` doc.

**File:** `frontend/src/components/AddScenarioModal.tsx`

4. **Pass `selectedBackend` to the hook:**
   ```typescript
   useScenarioUpload({
     ...existingProps,
     selectedBackend,
   });
   ```

**Note:** The SSE parsing in `uploadWithSSE` already handles the generic `data:` JSON format. The Fabric endpoint should emit the same `_sse_event("progress", {...})` format used by other provision endpoints — no frontend SSE changes needed.

---

## 3. Upload Jobs — Cosmos Persistence (V11E Task 8 Gap)

### Problem

`api/app/routers/upload_jobs.py` stores jobs in `_jobs: dict[str, dict]` (in-memory). Jobs vanish on server restart. The `ScenarioStatusPanel` polls `GET /api/upload-jobs` — it works for the current session but not across restarts or browser refreshes after a container restart.

### Architecture Decision

The `upload_jobs` router lives in the **api** service (port 8000), which does **not** have the Cosmos SDK. Two viable approaches:

**Option A — Proxy via graph-query-api (recommended):** Add a generic document CRUD endpoint to graph-query-api (e.g. `POST/GET/DELETE /query/docs/{container}/{id}`) and have `upload_jobs.py` call it with `httpx`. This follows the same proxy pattern the file already uses for upload steps.

**Option B — Add Cosmos SDK to api service:** Add `azure-cosmos` to `api/pyproject.toml` and create a minimal store. This duplicates Cosmos connection logic.

**Go with Option A** — it keeps Cosmos access centralized in graph-query-api.

### What To Build

**File:** `graph-query-api/` — new generic doc endpoint (or extend an existing router)

Add a minimal CRUD router at `/query/docs/{container_name}`:
- `GET /query/docs/{container}` — list docs (accepts `query` param)
- `GET /query/docs/{container}/{id}` — get doc by ID
- `PUT /query/docs/{container}/{id}` — upsert doc
- `DELETE /query/docs/{container}/{id}` — delete doc

Uses `get_document_store("scenarios", container_name, "/id", ensure_created=True)`.

**File:** `api/app/routers/upload_jobs.py`

1. **Add Cosmos proxy helpers:**
   ```python
   async def _persist_job(job: dict):
       """Write job to Cosmos via graph-query-api proxy."""
       async with httpx.AsyncClient(timeout=30) as client:
           await client.put(
               f"{GRAPH_QUERY_API}/query/docs/upload-jobs/{job['id']}",
               json=job,
           )

   async def _load_jobs() -> list[dict]:
       """Load all jobs from Cosmos."""
       async with httpx.AsyncClient(timeout=30) as client:
           resp = await client.get(f"{GRAPH_QUERY_API}/query/docs/upload-jobs")
           return resp.json().get("items", [])
   ```

2. **Write-through cache:** Keep `_jobs` dict as a fast cache. After `_update_step()`, call `_persist_job(job)`. In GET endpoints, serve from `_jobs` if populated, else load from Cosmos.

3. **Startup recovery:** In `create_upload_job()` or as a startup hook, load jobs from Cosmos, mark any `status: "running"` as `status: "error"` with `error: "Server restarted"`.

4. **Add `backend` + `workspace_id` fields to job doc:**
   ```python
   @router.post("")
   async def create_upload_job(
       scenario_name: str = Form(...),
       backend: str = Form("cosmosdb-gremlin"),    # new
       workspace_id: str = Form(""),                # new
       workspace_name: str = Form(""),               # new
       # ... existing file params
   ):
   ```
   Store in job doc. In `_run_upload_job()`:
   ```python
   if step_name == "graph" and job.get("backend") == "fabric-gql":
       # Route to Fabric pipeline (local call since both are in api service)
       endpoint = "http://127.0.0.1:8000/api/fabric/provision/graph"
       # POST as multipart with workspace_id, workspace_name
   else:
       endpoint = f"{GRAPH_QUERY_API}/query/upload/graph"
   ```

---

## 4. Modal → Background Job Submission (V11E Task 12)

### Problem

`AddScenarioModal` runs uploads synchronously via `useScenarioUpload` → `uploadWithSSE` per slot. The modal stays open for the entire duration (minutes). Close modal or refresh browser → upload dies.

### What To Change

**File:** `frontend/src/hooks/useScenarioUpload.ts`

Replace the sequential `uploadWithSSE` loop in `startUpload()` with a single multipart POST:

```typescript
const startUpload = useCallback(async () => {
  if (existingNames.includes(name) && !showOverrideConfirm) {
    setShowOverrideConfirm(true);
    return;
  }
  setShowOverrideConfirm(false);
  setModalState('uploading');
  setGlobalError('');

  try {
    const formData = new FormData();
    formData.append('scenario_name', name);
    formData.append('backend', selectedBackend || 'cosmosdb-gremlin');
    // workspace info for Fabric
    if (selectedBackend === 'fabric-gql') {
      formData.append('workspace_id', activeWorkspaceId);
      formData.append('workspace_name', activeWorkspaceName);
    }
    // Append each staged file
    for (const def of SLOT_DEFS) {
      const slot = slots[def.key];
      if (slot.file) formData.append(def.key, slot.file);
    }

    const resp = await fetch('/api/upload-jobs', { method: 'POST', body: formData });
    if (!resp.ok) throw new Error(`Upload failed: ${resp.status}`);

    const { job_id } = await resp.json();
    // Job submitted — close modal, ScenarioStatusPanel tracks progress
    onSaved();
    onClose();
  } catch (e) {
    setGlobalError(String(e));
    setModalState('error');
  }
}, [/* deps */]);
```

**Key changes:**
- `startUpload()` body shrinks from ~80 lines to ~25 lines
- No more per-slot SSE streaming, timer, abort controller
- Modal closes immediately after `POST /api/upload-jobs` returns `201`
- `ModalState` simplifies: only `'idle' | 'uploading' | 'error'` needed (the brief POST)
- `'saving'` and `'done'` states can be removed — the modal is gone before they'd trigger

**File:** `frontend/src/components/AddScenarioModal.tsx`

- Remove the per-slot progress bars shown during `modalState === 'uploading'`
- Remove the `✓ Scenario Saved` / done state UI
- Remove the timer display
- Keep: file staging, drag-and-drop, backend chooser, name display, override confirmation
- The "Save Scenario" button text stays; it just closes faster now

**Metadata save:** Currently `saveScenarioMeta()` runs after all uploads complete in the modal. Move this to the background job: after all steps finish in `_run_upload_job()`, proxy a call to save scenario metadata. Alternatively, the graph upload response already includes `scenario_metadata` — the background job can capture it from the graph step's response and persist it.

---

## 5. Implementation Order

Tasks should be done in this order due to dependencies:

| # | Task | File(s) | Depends On | Effort |
|---|------|---------|------------|--------|
| 1 | Add `POST /api/fabric/provision/graph` endpoint | `api/app/routers/fabric_provision.py` | — | 45 min |
| 2 | Add generic doc CRUD to graph-query-api | `graph-query-api/router_docs.py` (new), `main.py` | — | 20 min |
| 3 | Add Cosmos persistence to upload jobs | `api/app/routers/upload_jobs.py` | #2 | 30 min |
| 4 | Add `backend`/`workspace_id` to job creation + Fabric routing | `api/app/routers/upload_jobs.py` | #1, #3 | 20 min |
| 5 | Wire modal Save → `POST /api/upload-jobs`, strip progress UI | `useScenarioUpload.ts`, `AddScenarioModal.tsx` | #3, #4 | 30 min |
| 6 | Pass `selectedBackend` to hook, add workspace context | `AddScenarioModal.tsx`, `useScenarioUpload.ts` | #1 | 15 min |

---

## 6. File Changes

| File | Change |
|------|--------|
| `api/app/routers/fabric_provision.py` | Add `POST /provision/graph` — accepts tarball + workspace params, runs Lakehouse → Ontology → Graph Model pipeline via existing helpers |
| `graph-query-api/router_docs.py` | **New** — generic document CRUD at `/query/docs/{container}` using `get_document_store` |
| `graph-query-api/main.py` | Register `router_docs` |
| `api/app/routers/upload_jobs.py` | Cosmos persistence via proxy, `backend`/`workspace_id` fields, Fabric graph routing, startup recovery |
| `frontend/src/hooks/useScenarioUpload.ts` | Accept `selectedBackend`, replace sequential SSE uploads with single `POST /api/upload-jobs` |
| `frontend/src/components/AddScenarioModal.tsx` | Pass `selectedBackend` to hook, remove per-slot progress UI, remove done/timer states |

---

## 7. Risk Notes

1. **Scenario metadata save timing:** Currently runs in the modal after uploads complete. Moving to background job means metadata won't be saved until all steps finish. If the graph step returns `scenario_metadata` in its response, the background job should capture and persist it. Verify the graph upload endpoints return metadata in the SSE response body.

2. **Fabric endpoint concurrency:** The new `/provision/graph` reuses `_fabric_provision_lock`. If a full `/provision` is running, the graph tarball upload will get a 409. This is correct behavior — Fabric provisioning is inherently serial.

3. **SSE compatibility in background job:** The background job uses `httpx` and parses SSE text responses. The new Fabric endpoint returns SSE like existing endpoints — the parsing in `_run_upload_job()` already handles `data:` lines. No changes needed for that path.

4. **Generic doc endpoint security:** The `/query/docs/{container}` endpoint exposes arbitrary container CRUD. If this is a concern, restrict `container_name` to an allowlist (e.g. `["upload-jobs"]`).
