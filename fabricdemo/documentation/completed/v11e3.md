# V11E3 ‚Äî Fabric Backend Bug Fixes, Health Checks, Named Assets & Backend Isolation

> **Created:** 2026-02-17
> **Last audited:** 2026-02-17
> **Status:** ‚¨ú Not Started
> **Goal:** Fix four interconnected bugs that cause the `telco-noc-fabric` scenario
> to silently fall back to Cosmos endpoints, show blank screens when Fabric
> resources aren't provisioned, serve Gremlin/SQL prompts instead of GQL/KQL,
> and lack its own data generation scripts. Additionally: add data source health
> check cards to verify live connectivity, name Fabric assets per-scenario
> (SCENARIONAME-ontology, etc.), and prevent cross-backend scenario overwrites.

---

## Audit Log

| Date | Auditor | Summary |
|------|---------|---------|
| 2026-02-17 | Copilot | **Initial deep audit against codebase.** All 4 bugs confirmed via source. Fixed: line number references, upload routing plan (added `telemetry_backend` form param instead of piggybacking on `backend`), corrected AddScenarioModal name-field claim (editable, not read-only), fixed `config_store.py` function name (`save_scenario_config` not `update_scenario_config`), corrected `provision_graph_from_tarball` signature, fixed `ScenarioContext` field listing, reordered phases (P0 first ‚Äî backend isolation moved to Phase 3), added `scenario_name` pass-through to Fabric graph provision call. |

---

## Requirements (Original)

1. **Graph topology routing bug:** Graph topology is coming from Cosmos Gremlin instead of telco-noc-fabric's Fabric ontology, even though the scenario is telco-noc-fabric. Also ‚Äî the ontology isn't provisioned yet and the graph doesn't yet exist. So what would happen then? Blank screen? You'll have to grab all the graph data from the fabric ontology to visualize it, so what happens when no data is available?

2. **Telemetry upload routing bug:** Scenario upload shows telemetry completed, but there's no existing eventhouse in the Fabric workspace. So where did it go? Cosmos NoSQL?

3. **Prompts language mismatch bug:** Why are the prompts for telco-noc-fabric in Gremlin and Cosmos SQL instead of GQL and KQL? And why does that scenario not have its own data generation scripts? Reference `/home/hanchoong/projects/autonomous-network-demo/fabric_implementation_references` for language guidance. The old prompts for graph explorer are in `fabric_implementation_references/data/prompts/deprecated/fabric_network_data_agent_instructions.md` and `fabric_implementation_references/data/prompts/deprecated/fabric_telemetry_data_agent_instructions.md`.

4. **API tool routing bug (implied):** Make sure the API these agents are using is actually going to send KQL and GQL and not Gremlin.

5. **Data source health check cards:** For scenarios, can we have a health check on each service to prove that they are queryable? Reuse the agents bar asset and position it just under. It should have cards for each datasource, i.e. CosmosDB ‚Äì telco-noc-topology, Fabric ‚Äì telco-noc-ontology, etc. An indicator goes green when a query health check returns a successful response and red when not successful. When you hover over the cards, you should see the exact query request sent to each datasource, so you can verify that, for example, GQL was sent to fabric ontology, gremlin to cosmosdb gremlin, etc.

6. **Named Fabric assets (per-scenario):** Assets provisioned in the Fabric workspace are currently generic (`NetworkTopologyLH`, `NetworkTopologyOntology`, `NetworkTelemetryEH`). We want `SCENARIONAME-ontology`, `SCENARIONAME-lakehouse`, and `SCENARIONAME-eventhouse`, and we need to query the appropriate one based on scenario selection.

7. **Scenario backend isolation (moved from v11f):** Uploading a scenario with one backend should not silently overwrite the same-named scenario on a different backend. `telco-noc` (Cosmos) and `telco-noc` (Fabric) must be treated as separate scenarios. If there's a conflict, return HTTP 409 with a suggestion.

---

## Implementation Status

| Phase | Status | Scope | Priority |
|-------|--------|-------|----------|
| **Phase 1:** Prompt & data gen fixes (telco-noc-fabric) | ‚¨ú Not started | `data/scenarios/telco-noc-fabric/` prompts + scripts | P0 |
| **Phase 2:** Telemetry upload routing for Fabric | ‚¨ú Not started | `upload_jobs.py`, `fabric_provision.py`, `scenario.yaml` | P0 |
| **Phase 3:** Scenario backend isolation | ‚¨ú Not started | `router_scenarios.py`, `useScenarioUpload.ts`, `AddScenarioModal.tsx` | P0 |
| **Phase 4:** Graph topology fallback hardening + empty state | ‚¨ú Not started | `config.py` (graph-query-api), `GraphTopologyViewer.tsx` | P1 |
| **Phase 5:** Data source health check cards | ‚¨ú Not started | New `DataSourceBar.tsx`, `GET /query/health/sources`, `GraphBackend.ping()` | P1 |
| **Phase 6:** Named Fabric assets (per-scenario) | ‚¨ú Not started | `fabric_config.py`, `fabric_provision.py`, `scenario.yaml`, `FabricGQLBackend` | P1 |

### Deviations From Plan

| # | Plan Said | What Was Done | Rationale |
|---|-----------|---------------|-----------|
| D-1 | ‚Äî | ‚Äî | ‚Äî |

### Extra Work Not In Plan

- {None yet}

---

## Table of Contents

- [Requirements (Original)](#requirements-original)
- [Root Cause Analysis](#root-cause-analysis)
- [Bug 1: Graph Topology Falls Back to Cosmos](#bug-1-graph-topology-falls-back-to-cosmos)
- [Bug 2: Telemetry Upload Goes to Cosmos NoSQL](#bug-2-telemetry-upload-goes-to-cosmos-nosql)
- [Bug 3: Prompts Use Wrong Query Languages](#bug-3-prompts-use-wrong-query-languages)
- [Bug 4: API Tool Routing Verification](#bug-4-api-tool-routing-verification)
- [Item 5: Data Source Health Check Cards](#item-5-data-source-health-check-cards)
- [Item 6: Named Fabric Assets (Per-Scenario)](#item-6-named-fabric-assets-per-scenario)
- [Item 7: Scenario Backend Isolation](#item-7-scenario-backend-isolation)
- [Implementation Phases](#implementation-phases)
- [File Change Inventory](#file-change-inventory)
- [Edge Cases & Validation](#edge-cases--validation)
- [Migration & Backwards Compatibility](#migration--backwards-compatibility)

---

## Root Cause Analysis

All four bugs stem from a single root problem: **`telco-noc-fabric` is a shallow copy of `telco-noc` (Cosmos) with only the graph connector changed.** The prompts, data generation scripts, and telemetry routing were never adapted for Fabric.

```
telco-noc/                    telco-noc-fabric/
‚îú‚îÄ‚îÄ data/prompts/             ‚îú‚îÄ‚îÄ data/prompts/       ‚Üê COPIES, NOT ADAPTED
‚îÇ   ‚îú‚îÄ‚îÄ graph_explorer/       ‚îÇ   ‚îú‚îÄ‚îÄ graph_explorer/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ language_gremlin  ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ language_gremlin  ‚Üê BUG: should be language_gql
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ language_mock     ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ language_mock
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ...               ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ...               (language_gql.md MISSING)
‚îÇ   ‚îî‚îÄ‚îÄ foundry_telemetry‚Ä¶v2  ‚îÇ   ‚îî‚îÄ‚îÄ foundry_telemetry‚Ä¶v2  ‚Üê BUG: Cosmos SQL, should be KQL
‚îú‚îÄ‚îÄ scripts/                  ‚îú‚îÄ‚îÄ (NO scripts/)       ‚Üê BUG: no data gen
‚îÇ   ‚îú‚îÄ‚îÄ generate_topology.py  ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ generate_telemetry.py ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ ...                   ‚îÇ
‚îî‚îÄ‚îÄ scenario.yaml             ‚îî‚îÄ‚îÄ scenario.yaml
    graph: cosmosdb-gremlin       graph: fabric-gql    ‚úì correct
    telemetry: cosmosdb-nosql     telemetry: cosmosdb-nosql  ‚Üê BUG: should be fabric-kql
```

**Verified against codebase:** `data/scenarios/telco-noc-fabric/data/prompts/graph_explorer/` contains `core_instructions.md`, `core_schema.md`, `description.md`, `language_gremlin.md`, `language_mock.md` ‚Äî no `language_gql.md`. The `scripts/` directory is absent entirely.

---

## Bug 1: Graph Topology Falls Back to Cosmos

### Current State

**The routing logic works correctly in principle** ‚Äî `graph-query-api/config.py` L82-86 maps connectors via `CONNECTOR_TO_BACKEND` and the `get_scenario_context()` function (L96-145) reads the scenario's `data_sources.graph.connector` from the config store and dispatches to `FabricGQLBackend` or `CosmosDBGremlinBackend`.

**But** the entire config store lookup is wrapped in a silent `try/except Exception: pass`:

```python
# graph-query-api/config.py L118-136
backend_type = GRAPH_BACKEND  # default = "cosmosdb" (from env var, L29)
telemetry_backend_type = "cosmosdb-nosql"  # default
try:
    from config_store import fetch_scenario_config
    config = await fetch_scenario_config(prefix)
    connector = (
        config.get("data_sources", {})
              .get("graph", {})
              .get("connector", "")
    )
    if connector:
        backend_type = CONNECTOR_TO_BACKEND.get(connector, connector)
    tel_connector = (
        config.get("data_sources", {})
              .get("telemetry", {})
              .get("connector", "")
    )
    if tel_connector:
        telemetry_backend_type = TELEMETRY_CONNECTOR_MAP.get(tel_connector, "cosmosdb-nosql")
except Exception:
    pass  # ‚Üê SILENT FALLBACK TO COSMOSDB (L136)
```

If `fetch_scenario_config()` fails for **any** reason:
- Config store not populated yet (scenario not uploaded/saved)
- Prefix derivation wrong (`"telco-noc-fabric-topology"` ‚Üí `"telco-noc-fabric"` ‚Äî this part works)
- Cosmos DB temporarily unavailable
- Network error

‚Ä¶it silently falls back to `GRAPH_BACKEND = "cosmosdb"`. **No logging, no warning.**

### What the user sees

**Scenario:** User sets active scenario to `telco-noc-fabric`. Graph header is `telco-noc-fabric-topology`. Config store lookup fails. Backend falls back to Cosmos Gremlin ‚Üí queries `telco-noc-fabric-topology` graph in Cosmos Gremlin ‚Üí graph doesn't exist or returns data from a Cosmos graph that does exist with a similar name ‚Üí user sees the Cosmos data, not Fabric data.

**When Fabric ontology isn't provisioned:** Even if routing correctly goes to Fabric, if the ontology/graph model doesn't exist, `FabricGQLBackend.get_topology()` will fail or return `{"nodes": [], "edges": []}`. The frontend has **no empty-state message** ‚Äî it shows:
- `loading && data.nodes.length === 0` ‚Üí "Loading topology‚Ä¶" pulse animation (L112-116)
- After fetch completes with 0 nodes: loading becomes false, the loading message disappears
- Result: **blank dark canvas** with toolbar showing `0 nodes, 0 edges`
- No "Graph not provisioned" or "No data available" message

### Target State

1. **Add logging** to the fallback path so silent Cosmos fallback is visible in logs
2. **Add empty-state message** to `GraphTopologyViewer.tsx` when no nodes are returned after loading completes
3. **Add Fabric-specific error guidance** when Fabric backend returns 503 (not configured)

### Fix: `graph-query-api/config.py` L136 ‚Äî Add warning log on fallback

```python
# Replace the bare except (L135-136):
except Exception as exc:
    import logging
    logging.getLogger(__name__).warning(
        "Could not load scenario config for '%s', falling back to "
        "graph=%s / telemetry=%s: %s",
        prefix, GRAPH_BACKEND, "cosmosdb-nosql", exc,
    )
```

### Fix: `GraphTopologyViewer.tsx` L117 ‚Äî Add empty-state message

After the loading block (L112-116), add a post-load empty state:

```tsx
{/* Existing loading state ‚Äî L112-116 */}
{loading && data.nodes.length === 0 && (
  <div className="flex-1 flex items-center justify-center">
    <span className="text-xs text-text-muted animate-pulse">Loading topology‚Ä¶</span>
  </div>
)}

{/* NEW ‚Äî post-load empty state */}
{!loading && !error && data.nodes.length === 0 && (
  <div className="flex-1 flex flex-col items-center justify-center gap-2">
    <span className="text-sm text-text-muted">‚óá No graph data available</span>
    <span className="text-xs text-text-muted/60">
      The graph has not been provisioned yet, or the ontology is empty.
      Upload scenario data or run provisioning to populate the graph.
    </span>
  </div>
)}
```

### Fix: Handle 503 from Fabric backend

When `FabricGQLBackend` returns 503 (env vars not configured), the error message should include "Fabric environment variables not configured" rather than a generic HTTP error. Check if `backends/fabric.py` already surfaces a clear message ‚Äî if not, add one.

---

## Bug 2: Telemetry Upload Goes to Cosmos NoSQL

### Current State

Two compounding issues:

**Issue A ‚Äî `upload_jobs.py` has no Fabric telemetry routing:**

The upload routing (L188-222) has **one** `backend` variable per job, set from the `Form` parameter at L279 (default: `"cosmosdb-gremlin"`). This variable represents the **graph** backend. There is no separate `telemetry_backend` parameter.

```python
# upload_jobs.py L170: Graph backend variable
backend = job.get("backend", "cosmosdb-gremlin")

# L188-222: Only graph has Fabric routing
try:
    if step_name == "graph" and backend == "fabric-gql":
        endpoint = f"{API_SELF}/api/fabric/provision/graph"
        # ... Fabric graph handling (with workspace params)
    else:
        # Standard graph-query-api upload ‚Äî ALL other steps
        if step_name == "graph":
            endpoint = f"{GRAPH_QUERY_API}/query/upload/graph"
        elif step_name == "telemetry":
            endpoint = f"{GRAPH_QUERY_API}/query/upload/telemetry"  # ‚Üê ALWAYS COSMOS
        elif step_name == "runbooks":
            endpoint = f"{GRAPH_QUERY_API}/query/upload/runbooks"
        # ...
```

The `backend == "fabric-gql"` check only exists for the `graph` step. Telemetry **always** goes to `{GRAPH_QUERY_API}/query/upload/telemetry`, which is the Cosmos NoSQL ingest endpoint.

> **‚ö†Ô∏è PLAN BUG (fixed in this audit):** The original plan proposed `elif step_name == "telemetry" and backend == "fabric-gql"` ‚Äî but `backend` is the **graph** backend, not telemetry. Using graph backend as a proxy for telemetry routing is fragile and would break if someone wanted Fabric graph + Cosmos telemetry (a valid config). The fix below adds a **separate `telemetry_backend` Form parameter** to the upload job.

**Issue B ‚Äî `scenario.yaml` declares `cosmosdb-nosql` for telemetry:**

```yaml
# data/scenarios/telco-noc-fabric/scenario.yaml L64-65
telemetry:
    connector: "cosmosdb-nosql"  # ‚Üê Wrong for full Fabric scenario
```

Even the scenario manifest says Cosmos for telemetry. The full Fabric provision pipeline in `fabric_provision.py` has Eventhouse/KQL code (`_find_or_create_eventhouse` L355, `_create_kql_tables` L510, `_ingest_kql_data` L538) but it's gated on `telemetry_connector == "fabric-kql"` ‚Äî which this scenario never declares.

**Result:** Telemetry upload completes "successfully" ‚Äî it upserts all the telemetry docs into Cosmos NoSQL containers (`telco-noc-fabric-AlertStream`, `telco-noc-fabric-LinkTelemetry`). The user sees "Done" but no Eventhouse appears in Fabric because nothing was sent to Fabric.

```yaml
# data/scenarios/telco-noc-fabric/scenario.yaml L62
telemetry:
    connector: "cosmosdb-nosql"  # ‚Üê Wrong for full Fabric scenario
```

Even the scenario manifest says Cosmos for telemetry. The full Fabric provision pipeline in `fabric_provision.py` has Eventhouse/KQL code (`_find_or_create_eventhouse`, `_create_kql_tables`, `_ingest_kql_data`) but it's gated on `telemetry_connector == "fabric-kql"` ‚Äî which this scenario never declares.

**Result:** Telemetry upload completes "successfully" ‚Äî it upserts all the telemetry docs into Cosmos NoSQL containers (`telco-noc-fabric-AlertStream`, `telco-noc-fabric-LinkTelemetry`). The user sees "Done" but no Eventhouse appears in Fabric because nothing was sent to Fabric.

### Target State

**Decision point ‚Äî two approaches:**

#### Option A: Full Fabric telemetry (Eventhouse + KQL) ‚Äî RECOMMENDED

Change `scenario.yaml` to `telemetry.connector: "fabric-kql"`. Add telemetry routing in `upload_jobs.py` to call the Fabric provision pipeline's telemetry code. This gives a true end-to-end Fabric experience.

**Pros:** Complete Fabric story ‚Äî graph in ontology, telemetry in Eventhouse
**Cons:** Requires Eventhouse provisioning before telemetry upload; more complex

#### Option B: Keep Cosmos telemetry, make it transparent

Keep telemetry in Cosmos NoSQL but make the upload status and scenario metadata clearly indicate "telemetry ‚Üí Cosmos NoSQL" even for Fabric graph scenarios.

**Pros:** Simpler; works today
**Cons:** Doesn't demonstrate full Fabric capabilities

### Planned Fix (Option A ‚Äî Full Fabric)

#### 2A. `data/scenarios/telco-noc-fabric/scenario.yaml` ‚Äî Change telemetry connector

```yaml
# Before (L64-65):
telemetry:
    connector: "cosmosdb-nosql"

# After:
telemetry:
    connector: "fabric-kql"
    config:
      workspace_id: "${FABRIC_WORKSPACE_ID}"
      eventhouse_name: "telco-noc-fabric"
```

#### 2B. `api/app/routers/upload_jobs.py` ‚Äî Add `telemetry_backend` Form param + routing

**Step 1: Add Form parameter** (at L279, alongside existing `backend` param):

```python
# Existing:
backend: str = Form("cosmosdb-gremlin"),
# Add:
telemetry_backend: str = Form("cosmosdb-nosql"),
```

**Step 2: Store in job** (in `_make_job()` at L115, add `telemetry_backend` to the job dict):

```python
def _make_job(
    scenario_name: str, step_names: list[str], *,
    backend: str = "cosmosdb-gremlin",
    telemetry_backend: str = "cosmosdb-nosql",  # ‚Üê NEW
    workspace_id: str = "",
    workspace_name: str = "",
) -> dict:
    return {
        # ... existing fields ...
        "backend": backend,
        "telemetry_backend": telemetry_backend,  # ‚Üê NEW
        # ...
    }
```

**Step 3: Read in runner** (at L170):

```python
backend = job.get("backend", "cosmosdb-gremlin")
telemetry_backend = job.get("telemetry_backend", "cosmosdb-nosql")  # ‚Üê NEW
```

**Step 4: Add Fabric telemetry routing** (after the graph Fabric check, L188-200):

```python
if step_name == "graph" and backend == "fabric-gql":
    endpoint = f"{API_SELF}/api/fabric/provision/graph"
    # ... existing Fabric graph handling ...

elif step_name == "telemetry" and telemetry_backend == "fabric-kql":
    # ‚Üê NEW: Fabric telemetry routing (separate from graph backend)
    endpoint = f"{API_SELF}/api/fabric/provision/telemetry"
    with open(file_path, "rb") as f:
        resp = await client.post(
            endpoint,
            files={"file": (file_path.name, f, "application/gzip")},
            data={
                "workspace_id": workspace_id,
                "workspace_name": workspace_name,
                "scenario_name": scenario_name,
            },
            timeout=600,
        )

else:
    # Standard graph-query-api upload (existing code)
    ...
```

> **Why `telemetry_backend` instead of reusing `backend`?** The `backend` field is the **graph** backend (`cosmosdb-gremlin` or `fabric-gql`). A scenario may use Fabric for graph and Cosmos for telemetry, or vice versa. Decoupling them prevents unintended routing when backends differ per data source type.

**Step 5: Frontend ‚Äî pass telemetry_backend in upload** (in `useScenarioUpload.ts` or `AddScenarioModal.tsx`):

The frontend currently sends `backend` from `selectedBackend`. It also needs to read the scenario config's `data_sources.telemetry.connector` and pass it as `telemetry_backend`. The simplest approach: when `selectedBackend === 'fabric-gql'`, default `telemetry_backend` to `'fabric-kql'` (since full-Fabric scenarios use both). Allow override if mixed configs are needed later.

#### 2C. `api/app/routers/fabric_provision.py` ‚Äî New endpoint: `POST /api/fabric/provision/telemetry`

Extract the existing Eventhouse provisioning logic from the monolithic `/api/fabric/provision` pipeline into a standalone endpoint. The existing helper functions are already implemented:

| Helper Function | Line | Purpose |
|----------------|------|---------|
| `_find_or_create_eventhouse()` | L355 | Find/create Eventhouse in Fabric workspace |
| `_discover_kql_database()` | L495 | Get KQL DB connection from Eventhouse |
| `_create_kql_tables()` | L510 | Create KQL tables for telemetry schema |
| `_ingest_kql_data()` | L538 | Ingest data into KQL tables |

```python
@router.post("/fabric/provision/telemetry")
async def provision_telemetry_fabric(
    file: UploadFile = File(...),
    workspace_id: str = Form(...),
    workspace_name: str = Form(""),
    scenario_name: str = Form(""),
):
    """Provision Eventhouse + KQL tables and ingest telemetry from tarball."""
    async def generate():
        yield _sse_event("progress", step="telemetry", detail="Finding or creating Eventhouse‚Ä¶", pct=10)
        eh = await _find_or_create_eventhouse(workspace_id, scenario_name)
        
        yield _sse_event("progress", step="telemetry", detail="Discovering KQL database‚Ä¶", pct=20)
        kql_db = await _discover_kql_database(workspace_id, eh["id"])
        
        yield _sse_event("progress", step="telemetry", detail="Creating KQL tables‚Ä¶", pct=40)
        await _create_kql_tables(kql_db["connection_string"], scenario_name)
        
        yield _sse_event("progress", step="telemetry", detail="Ingesting telemetry data‚Ä¶", pct=60)
        # Extract CSV files from tarball, call _ingest_kql_data for each
        
        yield _sse_event("complete", message="Telemetry provisioned to Eventhouse")
    
    return StreamingResponse(generate(), media_type="text/event-stream")
```

> **Implementation note:** This follows the same pattern as `provision_graph_from_tarball` (L1216), which uses `Form(...)` parameters (not a Pydantic request model). The actual `provision_graph_from_tarball` signature is: `file: UploadFile = File(...)`, `workspace_id: str = Form(...)`, `workspace_name: str = Form(...)`, `lakehouse_name: str = Form("lakehouse")`, `ontology_name: str = Form("ontology")`.

#### 2D. `api/app/routers/upload_jobs.py` L192 ‚Äî Also pass `scenario_name` for graph provision

The existing Fabric graph provision call (L192-200) does **not** pass `scenario_name`:

```python
# Current (L192-200):
data={
    "workspace_id": workspace_id,
    "workspace_name": workspace_name,
},

# Fixed ‚Äî add scenario_name for per-scenario asset naming (Phase 6):
data={
    "workspace_id": workspace_id,
    "workspace_name": workspace_name,
    "scenario_name": scenario_name,  # ‚Üê NEW
},
```

---

## Bug 3: Prompts Use Wrong Query Languages

### Current State

The `telco-noc-fabric` scenario prompts are copies of `telco-noc` (Cosmos) that were never adapted. Two problems:

| File | Current Language | Needed Language |
|------|-----------------|-----------------|
| `data/scenarios/telco-noc-fabric/data/prompts/graph_explorer/language_gremlin.md` | **Gremlin** (`g.V().hasLabel()`, `.out()`, `.valueMap(true)`) | **GQL** (`MATCH`, `WHERE`, `RETURN`) |
| `data/scenarios/telco-noc-fabric/data/prompts/foundry_telemetry_agent_v2.md` | **Cosmos SQL** (`SELECT c.X FROM c WHERE c.Y = 'Z'`) | **KQL** (`TableName \| where Y == 'Z' \| project X`) |

**Additionally:** `language_gql.md` does NOT exist in `telco-noc-fabric/data/prompts/graph_explorer/`. The prompt composition logic in `graph-query-api/ingest/prompt_ingest.py` (L228-233) derives the language file name from the connector:

```python
# prompt_ingest.py L228-233
connector = _resolve_connector_for_agent(agent_def, scenario_config)
language_suffix = connector.split("-")[-1]  # fabric-gql ‚Üí "gql"
language_file = f"language_{language_suffix}.md"   # ‚Üí "language_gql.md"
```

Since `language_gql.md` doesn't exist, the composed prompt **has no query language instructions at all** ‚Äî the agent gets schema but no GQL syntax examples. The `language_gremlin.md` is correctly *not* included (filtered by connector mismatch), but nothing replaces it.

### The Correct Files Already Exist

| Need | Source (correct content) |
|------|------------------------|
| GQL language prompt | `fabric_implementation_references/data/prompts/graph_explorer/language_gql.md` (107 lines, full GQL syntax + examples) |
| KQL telemetry reference | `fabric_implementation_references/data/prompts/deprecated/fabric_telemetry_data_agent_instructions.md` (42 lines, Eventhouse schema with KQL types) |

### Planned Fix

#### 3A. Copy `language_gql.md` into telco-noc-fabric

```bash
cp fabric_implementation_references/data/prompts/graph_explorer/language_gql.md \
   data/scenarios/telco-noc-fabric/data/prompts/graph_explorer/language_gql.md
```

This file contains:
- GQL syntax rules (`MATCH (alias:Label)`, `WHERE`, `RETURN`)
- All 7 relationship query examples in GQL
- 3 multi-hop patterns (2-hop blast radius, 3-hop SLA exposure, router‚Üíswitch‚Üíbase station)
- Rules: never use `LOWER()`, never use `labels(n)`, never use `count(*)`

The existing `language_gremlin.md` can stay (it won't be included because `prompt_ingest.py` filters by connector ‚Äî only `language_gql.md` will be composed for fabric-gql scenarios).

#### 3B. Create `foundry_telemetry_agent_kql.md` for telco-noc-fabric

Write a new KQL-specific telemetry prompt based on:
- Schema from `fabric_implementation_references/data/prompts/deprecated/fabric_telemetry_data_agent_instructions.md`
- Structure from the existing `foundry_telemetry_agent_v2.md` (Cosmos version)

Key changes from Cosmos SQL ‚Üí KQL:

| Cosmos SQL (current) | KQL (needed) |
|---------------------|--------------|
| `SELECT c.AlertId, c.Timestamp FROM c WHERE c.Severity = 'CRITICAL' ORDER BY c.Timestamp DESC OFFSET 0 LIMIT 10` | `AlertStream \| where Severity == 'CRITICAL' \| top 10 by Timestamp desc \| project AlertId, Timestamp` |
| `FROM c` alias | Table name directly (e.g. `AlertStream`) |
| `c.PropertyName` | `PropertyName` (no alias needed) |
| `COUNT(1) AS cnt` | `\| summarize cnt = count() by AlertType` |
| `AVG(c.UtilizationPct)` | `\| summarize avgUtil = avg(UtilizationPct)` |
| `TOP N` | `\| top N by ...` or `\| take N` |
| Container name parameter | Table name directly in query |
| Types: `number`, `string` | Types: `real`, `datetime`, `string` |

The new file replaces `foundry_telemetry_agent_v2.md` for Fabric scenarios. It should:
1. Reference "Eventhouse tables" instead of "Cosmos DB containers"
2. Use KQL syntax for all example queries
3. Use KQL types (`real`, `datetime`) instead of JSON types (`number`, `string`)
4. Reference a `query_telemetry` tool that executes KQL queries against Eventhouse

#### 3C. Update `scenario.yaml` to point to correct telemetry prompt

The `scenario.yaml` agent definition for TelemetryAgent currently references:
```yaml
instructions_file: "prompts/foundry_telemetry_agent_v2.md"
```

Update to a Fabric-specific version:
```yaml
instructions_file: "prompts/foundry_telemetry_agent_kql.md"
```

Or use the existing compose-with-connector pattern if the prompt file naming supports it.

#### 3D. Data generation scripts for telco-noc-fabric

`telco-noc` has a `scripts/` directory with:
- `generate_all.sh`
- `generate_topology.py`
- `generate_telemetry.py`
- `generate_routing.py`
- `generate_tickets.py`

`telco-noc-fabric` has **no `scripts/` directory at all**.

**Options:**
1. **Symlink** (recommended for now): Since the underlying data is the same domain (same network topology, same telemetry patterns), create symlinks to the telco-noc scripts
2. **Copy + adapt**: If Fabric needs different data formats (e.g., CSV for Lakehouse instead of JSON for Cosmos), create adapted scripts

**Planned fix:**
```bash
mkdir -p data/scenarios/telco-noc-fabric/scripts
# Symlink since data content is identical ‚Äî just ingest format differs
ln -s ../../telco-noc/scripts/generate_all.sh data/scenarios/telco-noc-fabric/scripts/generate_all.sh
ln -s ../../telco-noc/scripts/generate_topology.py data/scenarios/telco-noc-fabric/scripts/generate_topology.py
ln -s ../../telco-noc/scripts/generate_telemetry.py data/scenarios/telco-noc-fabric/scripts/generate_telemetry.py
ln -s ../../telco-noc/scripts/generate_routing.py data/scenarios/telco-noc-fabric/scripts/generate_routing.py
ln -s ../../telco-noc/scripts/generate_tickets.py data/scenarios/telco-noc-fabric/scripts/generate_tickets.py
```

> **Note:** If Fabric telemetry ingest expects different CSV column types or naming, the generate scripts may need adaptation. For now, symlinks work because the CSV data generated by telco-noc is already consumed by the Fabric pipeline's `_ingest_kql_data()` function.

---

## Bug 4: API Tool Routing Verification

### Current State

**The API routing layer is actually correct:**

| Layer | Routing | Status |
|-------|---------|--------|
| `POST /query/graph` | Reads `data_sources.graph.connector` from config store ‚Üí dispatches to `FabricGQLBackend` or `CosmosDBGremlinBackend` | ‚úÖ Works correctly |
| `POST /query/telemetry` | Reads `data_sources.telemetry.connector` from config store ‚Üí dispatches to `FabricKQLBackend` or Cosmos NoSQL | ‚úÖ Works correctly |
| OpenAPI tool template | `{query_language_description}` substituted at provisioning time based on connector | ‚úÖ Works correctly |

**The problem is upstream of the API:**
1. The **agent's system prompt** has Gremlin examples (Bug 3), so it generates Gremlin queries ‚Üí sends them to the GQL backend ‚Üí Fabric rejects them
2. The **telemetry connector** is set to `cosmosdb-nosql` (Bug 2), so telemetry routing correctly goes to Cosmos ‚Äî the routing is right, the config is wrong

### Verification Steps

After fixing Bugs 2 and 3, verify:

1. **Graph tool:** Agent sends GQL query ‚Üí `POST /query/graph` ‚Üí `FabricGQLBackend.execute_query()` ‚Üí Fabric REST API ‚Üí results
2. **Telemetry tool:** Agent sends KQL query ‚Üí `POST /query/telemetry` ‚Üí `FabricKQLBackend` ‚Üí Eventhouse ‚Üí results
3. **OpenAPI description:** Provisioned tool description says "GQL" for graph, "KQL" for telemetry (check `CONNECTOR_OPENAPI_VARS` in `agent_provisioner.py`)

### One Gap Found: `CONNECTOR_OPENAPI_VARS` for telemetry KQL

Verified: `scripts/agent_provisioner.py` L53-96 has `CONNECTOR_OPENAPI_VARS` with three keys: `"cosmosdb"`, `"mock"`, `"fabric"`. The `"fabric"` key already maps `query_language_description` to GQL and `telemetry_query_language_description` to KQL. **No change needed ‚Äî already correct.**

```python
# scripts/agent_provisioner.py L53-96 ‚Äî verified, no change needed:
CONNECTOR_OPENAPI_VARS = {
    "fabric": {
        "query_language_description": "GQL (ISO Graph Query Language)...",
        "telemetry_query_language_description": "KQL (Kusto Query Language)...",
    },
    "cosmosdb": {
        "query_language_description": "Gremlin...",
        "telemetry_query_language_description": "Cosmos SQL...",
    },
}
```

---

## Item 5: Data Source Health Check Cards

### Current State

**No live data source health checking exists.** The existing `HealthDot` component in `AgentBar.tsx` polls `GET /health` (shallow ‚Äî returns `{status: "ok"}` without probing any backend). The frontend has `ServiceHealthSummary.tsx` and `ServiceHealthPopover.tsx` components that call `GET /api/services/health` (poll every 30s), but the endpoint **may not have a handler** ‚Äî the calls fail silently.

The `GraphBackend` protocol (`graph-query-api/backends/__init__.py` L22-74) defines `execute_query`, `get_topology`, `ingest`, and `close` ‚Äî **no `ping()` or `health_check()` method**. Note: `FabricKQLBackend` (in `backends/fabric_kql.py`, 82 lines) is telemetry-only ‚Äî it implements `execute_query()` and `close()` but NOT `get_topology()` or `ingest()`. The `ping()` addition for KQL would be on this partial-protocol class.

The user wants to **see at a glance which data sources are reachable and what exact queries are being sent** to each one (to verify GQL goes to Fabric, Gremlin to Cosmos, etc.).

### Target State

A horizontal strip below the AgentBar (same CSS pattern: `h-8 flex-shrink-0 bg-neutral-bg2 border-b`) showing one card per data source for the active scenario:

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ üü¢ CosmosDB Gremlin          üî¥ Fabric Ontology       üü¢ AI Search          ‚îÇ
‚îÇ    telco-noc-topology            telco-noc-ontology        telco-noc-runbooks ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Hover tooltip** shows the exact health check query used and the response:

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Fabric Ontology: telco-noc-ontology        ‚îÇ
‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ ‚îÇ
‚îÇ  Status: üî¥ Unreachable                     ‚îÇ
‚îÇ  Last checked: 19:03:42 (15s ago)           ‚îÇ
‚îÇ  Query sent:                                ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ MATCH (n) RETURN n LIMIT 1            ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ  Error: 503 ‚Äî Fabric environment not        ‚îÇ
‚îÇ         configured                          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Backend Changes

#### `GraphBackend` protocol ‚Äî Add `ping()` method

Add to `GraphBackend` in `backends/__init__.py`:

```python
async def ping(self) -> dict:
    """Health check ‚Äî returns {"ok": bool, "query": str, "detail": str, "latency_ms": int}"""
    ...
```

Implementations:
- **CosmosDBGremlinBackend:** `g.V().limit(1).count()` ‚Üí returns `{"ok": True, "query": "g.V().limit(1).count()", ...}`
- **FabricGQLBackend:** `MATCH (n) RETURN n LIMIT 1` ‚Üí returns `{"ok": True, "query": "MATCH (n) RETURN n LIMIT 1", ...}`
- **FabricKQLBackend:** `.show tables | take 1` ‚Üí returns `{"ok": True, "query": ".show tables | take 1", ...}`
- **MockBackend:** Always returns `{"ok": True, "query": "(mock)", ...}`

#### New endpoint: `GET /query/health/sources`

```python
@router.get("/health/sources")
async def health_check_sources(scenario: str = Query(...)):
    """Probe each data source defined in the scenario config and return health status."""
    config = await fetch_scenario_config(scenario)
    data_sources = config.get("data_sources", {})
    results = []
    
    for source_type, source_def in data_sources.items():
        connector = source_def.get("connector", "")
        resource_name = _derive_resource_name(source_type, source_def)
        backend = _get_backend_for_connector(connector)
        
        try:
            ping_result = await backend.ping(**source_def.get("config", {}))
            results.append({
                "source_type": source_type,       # "graph", "telemetry", "search_indexes.runbooks"
                "connector": connector,            # "fabric-gql", "cosmosdb-gremlin"
                "resource_name": resource_name,    # "telco-noc-ontology"
                "ok": ping_result["ok"],
                "query": ping_result["query"],
                "detail": ping_result.get("detail", ""),
                "latency_ms": ping_result.get("latency_ms", 0),
            })
        except Exception as exc:
            results.append({
                "source_type": source_type,
                "connector": connector,
                "resource_name": resource_name,
                "ok": False,
                "query": "(failed before query)",
                "detail": str(exc),
                "latency_ms": 0,
            })
    
    return {"sources": results, "checked_at": datetime.utcnow().isoformat()}
```

#### AI Search health check

For search indexes (`search_indexes.runbooks`, `search_indexes.tickets`), use the Azure AI Search `GET /indexes/{name}?api-version=...` endpoint to verify the index exists. No `ping()` needed ‚Äî just a metadata check.

### Frontend Changes

#### New component: `DataSourceBar.tsx`

Positioned in `Header.tsx` immediately after `<AgentBar />`:

```tsx
// Header.tsx:
<AgentBar />
<DataSourceBar />         {/* NEW */}
<ProvisioningBanner />
```

Component structure:

```tsx
export default function DataSourceBar() {
  const { activeScenario } = useScenario();
  const [sources, setSources] = useState<DataSourceHealth[]>([]);
  
  // Poll every 30 seconds
  useEffect(() => {
    if (!activeScenario) return;
    const check = () => fetch(`/query/health/sources?scenario=${activeScenario}`)
      .then(r => r.json()).then(d => setSources(d.sources)).catch(() => {});
    check();
    const iv = setInterval(check, 30_000);
    return () => clearInterval(iv);
  }, [activeScenario]);
  
  if (sources.length === 0) return null;
  
  return (
    <div className="h-8 flex-shrink-0 bg-neutral-bg2 border-b border-white/10
                    flex items-center gap-2 px-6 overflow-x-auto">
      <span className="text-[10px] text-text-muted uppercase tracking-wider mr-2">
        Sources
      </span>
      {sources.map(s => <DataSourceCard key={s.source_type} source={s} />)}
    </div>
  );
}
```

#### `DataSourceCard.tsx` ‚Äî Individual card with hover tooltip

```tsx
function DataSourceCard({ source }: { source: DataSourceHealth }) {
  const [showTooltip, setShowTooltip] = useState(false);
  
  return (
    <div className="relative" onMouseEnter={() => setShowTooltip(true)}
         onMouseLeave={() => setShowTooltip(false)}>
      <div className="flex items-center gap-1.5 px-2 py-1 rounded
                      bg-neutral-bg3 text-[10px]">
        <span className={source.ok ? 'text-green-400' : 'text-red-400'}>
          {source.ok ? '‚óè' : '‚óè'}
        </span>
        <span className="text-text-secondary">{connectorLabel(source.connector)}</span>
        <span className="text-text-muted"> ‚Äî {source.resource_name}</span>
      </div>
      
      {showTooltip && (
        <div className="absolute left-0 top-full mt-1 z-50 min-w-[260px]
                        bg-neutral-bg3 border border-white/10 rounded-lg
                        shadow-xl p-3 text-xs text-text-secondary">
          <p className="font-medium">{connectorLabel(source.connector)}: {source.resource_name}</p>
          <p className={`mt-1 ${source.ok ? 'text-green-400' : 'text-red-400'}`}>
            Status: {source.ok ? 'üü¢ Reachable' : 'üî¥ Unreachable'}
          </p>
          {source.latency_ms > 0 && (
            <p className="text-text-muted">Latency: {source.latency_ms}ms</p>
          )}
          <p className="mt-2 text-text-muted text-[10px] uppercase tracking-wider">
            Query sent:
          </p>
          <pre className="mt-1 p-2 bg-neutral-bg1 rounded text-[10px] font-mono
                          whitespace-pre-wrap">{source.query}</pre>
          {!source.ok && source.detail && (
            <p className="mt-2 text-red-400/80 text-[10px]">Error: {source.detail}</p>
          )}
        </div>
      )}
    </div>
  );
}

function connectorLabel(connector: string): string {
  const labels: Record<string, string> = {
    'cosmosdb-gremlin': 'CosmosDB Gremlin',
    'cosmosdb-nosql': 'CosmosDB NoSQL',
    'fabric-gql': 'Fabric Ontology',
    'fabric-kql': 'Fabric Eventhouse',
  };
  return labels[connector] || connector;
}
```

### UX Notes

- Poll interval: 30 seconds (not 15s like HealthDot ‚Äî these are heavier queries)
- The bar returns `null` and takes zero height when no sources are loaded
- Cards reflow horizontally with `overflow-x-auto` if many sources exist
- Search indexes show as "AI Search ‚Äî {index-name}" with a simple existence check

---

## Item 6: Named Fabric Assets (Per-Scenario)

### Current State

All Fabric assets use **generic default names** via env vars:

| Asset | Default Name | Source |
|-------|-------------|--------|
| Lakehouse | `NetworkTopologyLH` | `graph-query-api/adapters/fabric_config.py` L31 (`FABRIC_LAKEHOUSE_NAME`) |
| Eventhouse | `NetworkTelemetryEH` | `graph-query-api/adapters/fabric_config.py` L32 (`FABRIC_EVENTHOUSE_NAME`) |
| Ontology | `NetworkTopologyOntology` | `graph-query-api/adapters/fabric_config.py` L33 (`FABRIC_ONTOLOGY_NAME`) |

The provision request model (`api/app/routers/fabric_provision.py` L64-72 `FabricProvisionRequest`) accepts `lakehouse_name`, `eventhouse_name`, `ontology_name` overrides, but defaults are generic. **No logic to prefix with scenario name.** Note: The separate `provision_graph_from_tarball` endpoint (L1216) uses Form params with defaults of `"lakehouse"` and `"ontology"` ‚Äî these also need scenario-name prefixing.

Additionally, Fabric resource IDs (`FABRIC_WORKSPACE_ID`, `FABRIC_GRAPH_MODEL_ID`) are global env vars ‚Äî **not per-scenario**. `FabricGQLBackend` checks `kwargs` first (`workspace_id = kwargs.get("workspace_id") or FABRIC_WORKSPACE_ID`) but nothing passes per-scenario kwargs. The `ScenarioContext` dataclass has no Fabric-specific fields.

This means all Fabric scenarios share the same Lakehouse, Eventhouse, and Ontology. **Multiple Fabric scenarios cannot coexist.**

### Target State

Fabric asset names should follow the pattern `{SCENARIONAME}-{asset-type}`:

| Scenario | Lakehouse | Eventhouse | Ontology |
|----------|-----------|------------|----------|
| `telco-noc-fabric` | `telco-noc-fabric-lakehouse` | `telco-noc-fabric-eventhouse` | `telco-noc-fabric-ontology` |
| `energy-grid-fabric` | `energy-grid-fabric-lakehouse` | `energy-grid-fabric-eventhouse` | `energy-grid-fabric-ontology` |

Provisioned resource IDs (workspace_id, graph_model_id, eventhouse_id, lakehouse_id) should be stored per-scenario so queries can route to the correct assets.

### Backend Changes

#### `graph-query-api/adapters/fabric_config.py` ‚Äî Scenario-aware name resolution

Add a helper that derives asset names from scenario name:

```python
def fabric_asset_names(scenario_name: str) -> dict:
    """Derive per-scenario Fabric asset names."""
    return {
        "lakehouse_name": f"{scenario_name}-lakehouse",
        "eventhouse_name": f"{scenario_name}-eventhouse", 
        "ontology_name": f"{scenario_name}-ontology",
    }
```

The global env var defaults become fallbacks only when no scenario name is provided.

#### `api/app/routers/fabric_provision.py` ‚Äî Use scenario name for asset naming

Update the provision pipeline to derive asset names from `scenario_name` instead of using generic defaults:

```python
# Current:
FABRIC_LAKEHOUSE_NAME = os.getenv("FABRIC_LAKEHOUSE_NAME", "NetworkTopologyLH")

# New:
def _resolve_asset_name(override: str | None, scenario_name: str, asset_type: str) -> str:
    if override:
        return override
    if scenario_name:
        return f"{scenario_name}-{asset_type}"
    return os.getenv(f"FABRIC_{asset_type.upper()}_NAME", f"NetworkTopology{asset_type}")
```

In `provision_graph_from_tarball()`:
```python
lakehouse_name = _resolve_asset_name(req.lakehouse_name, scenario_name, "lakehouse")
ontology_name = _resolve_asset_name(req.ontology_name, scenario_name, "ontology")
```

#### `api/app/routers/upload_jobs.py` ‚Äî Pass scenario name to provision endpoints

When calling Fabric provision endpoints, include `scenario_name` so asset names can be derived:

```python
# Already passes workspace_id ‚Äî add scenario_name:
data={
    "workspace_id": workspace_id,
    "scenario_name": scenario_name,  # ‚Üê used for asset naming
}
```

#### Per-scenario resource ID storage

After provisioning, store the provisioned Fabric resource IDs in the scenario's config store document:

```python
# In fabric_provision.py, after successful provisioning:
# Use existing save_scenario_config() from config_store.py (not update_ ‚Äî that doesn't exist)
await config_store.save_scenario_config(scenario_name, {
    **existing_config,
    "fabric_resources": {
        "workspace_id": workspace_id,
        "lakehouse_id": lakehouse_id,
        "lakehouse_name": lakehouse_name,
        "ontology_name": ontology_name,
        "graph_model_id": graph_model_id,
        "eventhouse_id": eventhouse_id,      # if telemetry provisioned
        "eventhouse_name": eventhouse_name,   # if telemetry provisioned
    }
})
```

> **‚ö†Ô∏è Note:** `config_store.py` (61 lines) has `fetch_scenario_config()` and `save_scenario_config()` ‚Äî there is no `update_scenario_config()`. Use `save_scenario_config()` and merge with the existing config dict to avoid overwriting other fields.
```

#### `graph-query-api/config.py` ‚Äî Read per-scenario Fabric resource IDs

In `get_scenario_context()`, after loading scenario config, extract Fabric resource IDs:

```python
# After existing config store lookup:
fabric_resources = config.get("fabric_resources", {})
workspace_id = fabric_resources.get("workspace_id", FABRIC_WORKSPACE_ID)
graph_model_id = fabric_resources.get("graph_model_id", FABRIC_GRAPH_MODEL_ID)
```

Pass these through `ScenarioContext` ‚Üí `FabricGQLBackend.execute_query(workspace_id=..., graph_model_id=...)`.

#### `graph-query-api/config.py` ‚Äî Add Fabric fields to `ScenarioContext`

Current fields (L55-78):
```python
@dataclass
class ScenarioContext:
    graph_name: str                  # e.g. "cloud-outage-topology"
    graph_database: str              # e.g. "networkgraph"
    telemetry_database: str          # "telemetry"
    telemetry_container_prefix: str  # "cloud-outage"
    prompts_database: str            # "prompts"
    prompts_container: str           # "cloud-outage"
    backend_type: str                # "cosmosdb" or "fabric-gql"
    telemetry_backend_type: str      # "cosmosdb-nosql" or "fabric-kql"
```

Add Fabric-specific fields:
```python
    # NEW ‚Äî per-scenario Fabric routing (populated from config store's fabric_resources)
    fabric_workspace_id: str = ""
    fabric_graph_model_id: str = ""
    fabric_eventhouse_id: str = ""
```

#### `data/scenarios/telco-noc-fabric/scenario.yaml` ‚Äî Remove generic config refs

```yaml
# Before:
config:
  workspace_id: "${FABRIC_WORKSPACE_ID}"
  graph_model_id: "${FABRIC_GRAPH_MODEL_ID}"

# After:
config:
  workspace_id: "${FABRIC_WORKSPACE_ID}"   # Workspace stays global (one workspace for all)
  # graph_model_id resolved at provisioning time from ontology name
```

> **Note:** `workspace_id` can stay as a global env var (one Fabric workspace for the demo). The key change is that ontology/lakehouse/eventhouse names and their provisioned IDs become per-scenario.

### Migration

- Existing generic Fabric assets (`NetworkTopologyLH`, `NetworkTopologyOntology`, `NetworkTelemetryEH`) are unaffected ‚Äî they remain as fallbacks when no scenario name is provided.
- Re-provisioning `telco-noc-fabric` after this change will create new assets with scenario-prefixed names.
- Old generic assets become orphaned but harmless (can be manually deleted in Fabric workspace).

---

## Item 7: Scenario Backend Isolation

> **Moved from v11f.md Item 3**

### Current State

Scenario identity is `id = name` (plain string, L201: `"id": name`). When a user uploads `telco-noc` with Fabric backend:

1. `POST /api/upload-jobs` with `scenario_name="telco-noc"`, `backend="fabric-gql"`
2. Graph step routes to Fabric provisioning (correct)
3. `save_scenario()` (L173) upserts `{"id": "telco-noc", "graph_connector": "fabric-gql"}`
4. **This overwrites the existing CosmosDB `telco-noc` document** ‚Äî metadata now says Fabric, but the CosmosDB graph data (`telco-noc-topology`) is still there, orphaned

The `graph_connector` field resolution priority (L191-195):
```python
graph_connector = (
    req.graph_connector                          # explicit from request
    or (req.config or {}).get("data_sources", {}).get("graph", {}).get("connector", "")
    or "cosmosdb-gremlin"                        # fallback default
)
```

The upsert at L218 (`result = await store.upsert(doc)`) performs a blind write. **No backend conflict detection.**

### Target State

**Server-side conflict detection** in `save_scenario()` + **frontend warning**:

1. If `telco-noc` exists with `graph_connector: "cosmosdb-gremlin"` and you try to save `telco-noc` with `graph_connector: "fabric-gql"` ‚Üí **HTTP 409** with clear message
2. Frontend catches 409, shows suggestion: "telco-noc already exists as a Cosmos scenario. Use 'telco-noc-fabric' instead."
3. Same-backend re-upload still works (existing override confirmation handles this)

### Backend Changes

#### `graph-query-api/router_scenarios.py` ‚Äî Backend conflict check in `save_scenario()`

Add conflict detection inside the existing `try/except` block (L213-217). The current code:

```python
# Current (L213-217):
try:
    existing = await store.get(name, partition_key=name)
    if existing:
        doc["created_at"] = existing.get("created_at", now)
except Exception:
    pass  # Document doesn't exist yet ‚Äî OK
```

Replace with:

```python
# New (L213-222):
try:
    existing = await store.get(name, partition_key=name)
    if existing:
        doc["created_at"] = existing.get("created_at", now)
        # Backend conflict check ‚Äî prevent cross-backend overwrite
        existing_connector = existing.get("graph_connector", "cosmosdb-gremlin")
        if existing_connector != graph_connector:
            suggested = f"{name}-fabric" if graph_connector == "fabric-gql" else f"{name}-cosmos"
            raise HTTPException(
                409,
                detail={
                    "message": f"Scenario '{name}' already exists with backend '{existing_connector}'.",
                    "suggestion": f"Use a different name (e.g. '{suggested}') or delete the existing scenario first.",
                    "existing_backend": existing_connector,
                    "requested_backend": graph_connector,
                }
            )
except HTTPException:
    raise  # ‚ö†Ô∏è CRITICAL: Don't swallow the 409!
except Exception:
    pass  # Document doesn't exist yet ‚Äî fine
```

> **‚ö†Ô∏è TRAP:** The bare `except Exception: pass` at L216 would catch `HTTPException(409)` since `HTTPException` inherits from `Exception`. The `except HTTPException: raise` line **MUST** come before `except Exception`. This is the #1 implementation pitfall in this entire plan.

### Frontend Changes

#### `useScenarioUpload.ts` ‚Äî Handle 409 conflict from saveScenarioMeta

The current error handling (L161-173) **swallows all errors** with `console.warn`:

```typescript
// Current (L161-173):
try {
  await saveScenarioMeta({
    name,
    display_name: displayName || undefined,
    description: description || undefined,
    graph_connector: selectedBackend === 'fabric-gql' ? 'fabric-gql' : undefined,
    upload_results: {},
  });
} catch (e) {
  console.warn('Failed to save scenario metadata:', e);  // ‚Üê swallowed
}
```

Replace with 409-aware handling:

```typescript
try {
    await saveScenarioMeta({ ... });
} catch (e) {
    if (e instanceof Response && e.status === 409) {
        const body = await e.json().catch(() => ({}));
        const suggestion = body?.detail?.suggestion || 'Use a different name.';
        setGlobalError(
            `Backend conflict: ${body?.detail?.message || 'Scenario exists with a different backend.'} ${suggestion}`
        );
        setModalState('error');
        return;
    }
    console.warn('Failed to save scenario metadata:', e);
}
```

> **Note:** Check how `saveScenarioMeta` is implemented ‚Äî if it uses `fetch()`, the error may come as a non-OK response (not a thrown Error). Ensure the 409 check matches the actual error shape. If `saveScenarioMeta` throws on non-2xx, the `e.status === 409` or `e.message.includes('409')` pattern applies.

#### `AddScenarioModal.tsx` ‚Äî Proactive conflict warning (P1)

When Fabric backend is selected and the auto-detected name matches an existing Cosmos scenario:

```tsx
{selectedBackend === 'fabric-gql' && name && !name.endsWith('-fabric') &&
 existingNames.includes(name) && (
  <div className="bg-yellow-500/10 border border-yellow-500/30 rounded-lg p-3 text-xs">
    <p className="text-yellow-400 font-medium">‚ö† Name Conflict Risk</p>
    <p className="text-yellow-400/70 mt-1">
      "{name}" exists as a Cosmos scenario. Fabric scenarios should use a
      different name to avoid conflicts.
    </p>
    <button
      onClick={() => setName(`${name}-fabric`)}
      className="mt-2 px-2 py-1 bg-yellow-600/20 rounded text-yellow-300 hover:bg-yellow-600/30"
    >
      Use "{name}-fabric" instead
    </button>
  </div>
)}
```

> **Implementation note:** The name field is **editable** (`useState` at L65, with `setName` handler). It can be manually typed or auto-detected from dropped filenames (L127-140 `handleDrop` ‚Üí `setName`). Both options below are viable:
> - **(a)** Allow in-modal name override when conflict is detected ‚Äî natural since the field is already editable
> - **(b)** Require the tarballs to be named with the `-fabric` suffix (`telco-noc-fabric-graph.tar.gz`), which aligns with the existing data directory convention

---

## Implementation Phases

> **Ordering principle:** All P0 phases (1-3) execute first ‚Äî these fix active bugs that cause silent data corruption or agent failure. P1 phases (4-6) follow, with dependencies respected. Within each priority tier, phases are ordered for maximum parallelism (data-only changes before code changes, backend before frontend).

---

### Phase 1: Prompt & Data Gen Fixes (telco-noc-fabric)

> **Priority: P0** ‚Äî This is the root cause of agents generating wrong queries.
> **Dependencies: None**
> **Estimated effort: Small (mostly file copies + one new file)**

**Tasks:**

- [ ] **1.1** Copy `language_gql.md` from `fabric_implementation_references/data/prompts/graph_explorer/` to `data/scenarios/telco-noc-fabric/data/prompts/graph_explorer/`
- [ ] **1.2** Create `foundry_telemetry_agent_kql.md` with KQL syntax examples and Eventhouse schema (reference `fabric_implementation_references/data/prompts/deprecated/fabric_telemetry_data_agent_instructions.md`)
- [ ] **1.3** Update `scenario.yaml` TelemetryAgent `instructions_file` to reference the KQL prompt (L107: `instructions_file: "prompts/foundry_telemetry_agent_kql.md"`)
- [ ] **1.4** Create `data/scenarios/telco-noc-fabric/scripts/` directory with symlinks to telco-noc scripts

**Files:**
| File | Action | Lines Changed |
|------|--------|---------------|
| `data/scenarios/telco-noc-fabric/data/prompts/graph_explorer/language_gql.md` | CREATE | Copy from `fabric_implementation_references` (~107 lines) |
| `data/scenarios/telco-noc-fabric/data/prompts/foundry_telemetry_agent_kql.md` | CREATE | New KQL telemetry prompt (~120 lines) |
| `data/scenarios/telco-noc-fabric/scenario.yaml` | MODIFY | L107: change `instructions_file` |
| `data/scenarios/telco-noc-fabric/scripts/*` | CREATE | 5 symlinks to `../../telco-noc/scripts/` |

**Verification:**
- [ ] Upload telco-noc-fabric scenario ‚Üí check uploaded prompts in prompt store
- [ ] Graph explorer agent's composed prompt contains GQL syntax examples (not Gremlin)
- [ ] Telemetry agent's prompt references Eventhouse + KQL (not Cosmos + SQL)
- [ ] `generate_all.sh` runs from `telco-noc-fabric/scripts/`

---

### Phase 2: Telemetry Upload Routing for Fabric

> **Priority: P0** ‚Äî Without this, Fabric telemetry goes to Cosmos with no Eventhouse created.
> **Dependencies: None** (independent of Phase 1; Phase 1 changes the yaml prompt ref, this changes routing)
> **Estimated effort: Medium (new form param + routing branch + new endpoint)**

**Tasks:**

- [ ] **2.1** Change `scenario.yaml` telemetry connector from `cosmosdb-nosql` to `fabric-kql` (L64-65)
- [ ] **2.2** Add `telemetry_backend: str = Form("cosmosdb-nosql")` parameter to `create_upload_job()` (L279)
- [ ] **2.3** Add `telemetry_backend` to `_make_job()` dict (L115) and read it in `_run_upload_job()` (L170)
- [ ] **2.4** Add `elif step_name == "telemetry" and telemetry_backend == "fabric-kql"` routing branch (after L200)
- [ ] **2.5** Add `scenario_name` to existing Fabric graph provision `data={}` (L192-200)
- [ ] **2.6** Create `POST /api/fabric/provision/telemetry` endpoint in `fabric_provision.py` (wrapping existing helpers at L355, L495, L510, L538)
- [ ] **2.7** Update frontend `useScenarioUpload.ts` to pass `telemetry_backend` in FormData

**Files:**
| File | Action | Lines Changed |
|------|--------|---------------|
| `data/scenarios/telco-noc-fabric/scenario.yaml` | MODIFY | L64-65: telemetry connector |
| `api/app/routers/upload_jobs.py` | MODIFY | L115 (_make_job), L170 (read), L200+ (routing), L279 (form param) |
| `api/app/routers/fabric_provision.py` | MODIFY | New endpoint ~50 lines |
| `frontend/src/hooks/useScenarioUpload.ts` | MODIFY | Pass `telemetry_backend` in FormData |

**Verification:**
- [ ] Upload telco-noc-fabric ‚Üí telemetry step calls `/api/fabric/provision/telemetry`
- [ ] Eventhouse appears in Fabric workspace with `AlertStream` and `LinkTelemetry` tables
- [ ] Upload SSE shows: "Finding Eventhouse‚Ä¶", "Creating KQL tables‚Ä¶", etc.
- [ ] **Negative test:** telco-noc (Cosmos) telemetry upload still goes to Cosmos NoSQL
- [ ] **Negative test:** A Fabric-graph + Cosmos-telemetry scenario routes telemetry to Cosmos

---

### Phase 3: Scenario Backend Isolation

> **Priority: P0** ‚Äî Prevents silent data corruption from cross-backend overwrite.
> **Dependencies: None** (independent of Phases 1-2)
> **Estimated effort: Small (~15 lines backend + ~20 lines frontend)**

**Tasks:**

- [ ] **3.1** Add backend conflict check in `save_scenario()` in `router_scenarios.py` (L213-217)
- [ ] **3.2** Ensure `except HTTPException: raise` comes before `except Exception: pass` to avoid swallowing 409
- [ ] **3.3** Handle 409 response in `useScenarioUpload.ts` (L161-173)
- [ ] **3.4** Add proactive conflict warning banner in `AddScenarioModal.tsx` when Fabric selected + name matches existing Cosmos scenario

**Files:**
| File | Action | Lines Changed |
|------|--------|---------------|
| `graph-query-api/router_scenarios.py` | MODIFY | L213-217: 409 conflict check (~15 lines) |
| `frontend/src/hooks/useScenarioUpload.ts` | MODIFY | L161-173: 409 handling (~10 lines) |
| `frontend/src/components/AddScenarioModal.tsx` | MODIFY | After L65: warning banner (~15 lines) |

**Verification:**
- [ ] Upload "telco-noc" with Cosmos ‚Üí succeeds
- [ ] Upload "telco-noc" with Fabric ‚Üí **409 error** with suggestion to use "telco-noc-fabric"
- [ ] Upload "telco-noc-fabric" with Fabric ‚Üí succeeds as a separate scenario
- [ ] Re-upload "telco-noc" with Cosmos (same backend) ‚Üí override confirmation ‚Üí succeeds
- [ ] Delete "telco-noc" (Cosmos) ‚Üí then upload "telco-noc" with Fabric ‚Üí succeeds (no conflict)
- [ ] Verify 409 body has `detail.message` and `detail.suggestion` as structured JSON

---

### Phase 4: Graph Topology Fallback Hardening + Empty State

> **Priority: P1** ‚Äî UX improvement + debugging aid. Prevents silent misrouting and blank screens.
> **Dependencies: None**
> **Estimated effort: Tiny (~3 lines backend + ~8 lines frontend)**

**Tasks:**

- [ ] **4.1** Add `logger.warning()` to the `except Exception: pass` block in `graph-query-api/config.py` (L135-136)
- [ ] **4.2** Add empty-state message in `GraphTopologyViewer.tsx` after L116 when `!loading && !error && data.nodes.length === 0`
- [ ] **4.3** Check `backends/fabric.py` returns a descriptive 503 error when env vars are missing (not a generic HTTP error)

**Files:**
| File | Action | Lines Changed |
|------|--------|---------------|
| `graph-query-api/config.py` | MODIFY | L135-136: replace bare `pass` with `logger.warning()` (~3 lines) |
| `frontend/src/components/GraphTopologyViewer.tsx` | MODIFY | After L116: empty-state div (~8 lines) |

**Verification:**
- [ ] Set active scenario where config store lookup fails ‚Üí check logs for warning with prefix + backend
- [ ] Set active scenario with no graph data ‚Üí UI shows "No graph data available" message
- [ ] Normal scenario with data ‚Üí graph renders, no warning logged

---

### Phase 5: Data Source Health Check Cards

> **Priority: P1** ‚Äî Live proof that routing is correct; visual confirmation of GQL vs Gremlin.
> **Dependencies: Phase 1 + Phase 4** (prompts and fallback logging should be in place first)
> **Estimated effort: Medium (~250 lines across new components + endpoint + protocol changes)**

**Tasks:**

- [ ] **5.1** Add `ping()` method to `GraphBackend` protocol in `backends/__init__.py`
- [ ] **5.2** Implement `ping()` in `CosmosDBGremlinBackend` (`backends/cosmosdb.py`) ‚Äî `g.V().limit(1).count()`
- [ ] **5.3** Implement `ping()` in `FabricGQLBackend` (`backends/fabric.py`) ‚Äî `MATCH (n) RETURN n LIMIT 1`
- [ ] **5.4** Implement `ping()` in `FabricKQLBackend` (`backends/fabric_kql.py`) ‚Äî `.show tables | take 1`
- [ ] **5.5** Implement `ping()` in `MockGraphBackend` (`backends/mock.py`) ‚Äî always ok
- [ ] **5.6** Create `graph-query-api/router_health.py` with `GET /query/health/sources` endpoint
- [ ] **5.7** Add AI Search index health check (index existence via `GET /indexes/{name}`)
- [ ] **5.8** Create `frontend/src/components/DataSourceBar.tsx` ‚Äî horizontal strip with source cards
- [ ] **5.9** Create `frontend/src/components/DataSourceCard.tsx` ‚Äî individual card with hover tooltip
- [ ] **5.10** Add `<DataSourceBar />` in `Header.tsx` after `<AgentBar />` (L49)

**Files:**
| File | Action | Lines Changed |
|------|--------|---------------|
| `graph-query-api/backends/__init__.py` | MODIFY | Add `ping()` to protocol (~5 lines) |
| `graph-query-api/backends/cosmosdb.py` | MODIFY | Implement `ping()` (~10 lines) |
| `graph-query-api/backends/fabric.py` | MODIFY | Implement `ping()` (~10 lines) |
| `graph-query-api/backends/fabric_kql.py` | MODIFY | Implement `ping()` (~10 lines) |
| `graph-query-api/backends/mock.py` | MODIFY | Implement `ping()` (~5 lines) |
| `graph-query-api/router_health.py` | CREATE | `GET /query/health/sources` (~60 lines) |
| `frontend/src/components/DataSourceBar.tsx` | CREATE | ~60 lines |
| `frontend/src/components/DataSourceCard.tsx` | CREATE | ~50 lines |
| `frontend/src/components/Header.tsx` | MODIFY | L49: add `<DataSourceBar />` (~1 line) |

**Verification:**
- [ ] With telco-noc (Cosmos) active ‚Üí cards show CosmosDB Gremlin (green), AI Search (green)
- [ ] With telco-noc-fabric active (not provisioned) ‚Üí Fabric Ontology (red), Fabric Eventhouse (red)
- [ ] Hover over card ‚Üí tooltip shows exact query (e.g. `MATCH (n) RETURN n LIMIT 1` for Fabric)
- [ ] After Fabric provisioning ‚Üí cards turn green
- [ ] Cards auto-refresh every 30 seconds
- [ ] No active scenario ‚Üí DataSourceBar returns null, zero height

---

### Phase 6: Named Fabric Assets (Per-Scenario)

> **Priority: P1** ‚Äî Needed before multiple Fabric scenarios can coexist.
> **Dependencies: Phase 2** (telemetry routing must support Fabric before naming matters for Eventhouse)
> **Estimated effort: Medium (~50 lines across config, provision, and context changes)**

**Tasks:**

- [ ] **6.1** Add `fabric_asset_names()` helper to `graph-query-api/adapters/fabric_config.py`
- [ ] **6.2** Add `_resolve_asset_name()` helper to `api/app/routers/fabric_provision.py`
- [ ] **6.3** Update `provision_graph_from_tarball` (L1216) to use scenario name for asset naming
- [ ] **6.4** Update new `provision_telemetry_fabric` endpoint to use scenario name
- [ ] **6.5** Store `fabric_resources` in config store after provisioning (use `save_scenario_config()`)
- [ ] **6.6** Add Fabric fields (`fabric_workspace_id`, `fabric_graph_model_id`, `fabric_eventhouse_id`) to `ScenarioContext` (L55-78)
- [ ] **6.7** Read `fabric_resources` from config store in `get_scenario_context()` and pass to backends
- [ ] **6.8** Update `scenario.yaml` to remove hardcoded `graph_model_id` ref (resolved at provisioning time)

**Files:**
| File | Action | Lines Changed |
|------|--------|---------------|
| `graph-query-api/adapters/fabric_config.py` | MODIFY | Add `fabric_asset_names()` (~10 lines) |
| `api/app/routers/fabric_provision.py` | MODIFY | `_resolve_asset_name()` + use scenario names (~20 lines); store provisioned IDs (~10 lines) |
| `graph-query-api/config_store.py` | NO CHANGE | Use existing `save_scenario_config()` ‚Äî no new function needed |
| `graph-query-api/config.py` | MODIFY | L55-78: add Fabric fields to `ScenarioContext`; L120-140: read `fabric_resources` (~15 lines) |
| `data/scenarios/telco-noc-fabric/scenario.yaml` | MODIFY | Remove hardcoded `graph_model_id` |

**Verification:**
- [ ] Provision telco-noc-fabric ‚Üí assets created as `telco-noc-fabric-lakehouse`, `telco-noc-fabric-ontology`, `telco-noc-fabric-eventhouse`
- [ ] Query graph for telco-noc-fabric ‚Üí routes to `telco-noc-fabric-ontology` (not `NetworkTopologyOntology`)
- [ ] Provision a second Fabric scenario ‚Üí creates separate assets with different prefix
- [ ] Old generic assets still accessible as fallback when no scenario name provided

---

## File Change Inventory

| File | Action | Phase | Changes |
|------|--------|-------|---------|
| `data/scenarios/telco-noc-fabric/data/prompts/graph_explorer/language_gql.md` | CREATE | 1 | Copy from `fabric_implementation_references` (~107 lines) |
| `data/scenarios/telco-noc-fabric/data/prompts/foundry_telemetry_agent_kql.md` | CREATE | 1 | New KQL telemetry prompt (~120 lines) |
| `data/scenarios/telco-noc-fabric/scenario.yaml` | MODIFY | 1+2 | L107: update telemetry prompt ref; L64-65: change telemetry connector to `fabric-kql` |
| `data/scenarios/telco-noc-fabric/scripts/*` | CREATE | 1 | Symlinks to `../../telco-noc/scripts/` (5 files) |
| `api/app/routers/upload_jobs.py` | MODIFY | 2 | L115: add `telemetry_backend` to `_make_job()`; L170: read `telemetry_backend`; L200+: add Fabric telemetry routing branch; L192: add `scenario_name` to graph provision data; L279: add `telemetry_backend` Form param |
| `api/app/routers/fabric_provision.py` | MODIFY | 2+6 | New `POST /api/fabric/provision/telemetry` endpoint (~50 lines); `_resolve_asset_name()` helper; store provisioned IDs (~20 lines) |
| `frontend/src/hooks/useScenarioUpload.ts` | MODIFY | 2+3 | Pass `telemetry_backend` in FormData (~3 lines); handle 409 in saveScenarioMeta (~10 lines) |
| `graph-query-api/router_scenarios.py` | MODIFY | 3 | L213-217: backend conflict check ‚Üí 409 on connector mismatch; `except HTTPException: raise` guard (~15 lines) |
| `frontend/src/components/AddScenarioModal.tsx` | MODIFY | 3 | Proactive conflict warning banner (~15 lines) |
| `graph-query-api/config.py` | MODIFY | 4+6 | L135-136: replace bare `pass` with `logger.warning()` (~3 lines); L55-78: add Fabric fields to `ScenarioContext`; L120-140: read `fabric_resources` (~15 lines) |
| `frontend/src/components/GraphTopologyViewer.tsx` | MODIFY | 4 | After L116: empty-state message (~8 lines) |
| `graph-query-api/backends/__init__.py` | MODIFY | 5 | Add `ping()` to `GraphBackend` protocol (~5 lines) |
| `graph-query-api/backends/cosmosdb.py` | MODIFY | 5 | Implement `ping()` ‚Äî `g.V().limit(1).count()` (~10 lines) |
| `graph-query-api/backends/fabric.py` | MODIFY | 5+6 | Implement `ping()` (~10 lines); pass per-scenario `workspace_id`/`graph_model_id` |
| `graph-query-api/backends/fabric_kql.py` | MODIFY | 5 | Implement `ping()` ‚Äî `.show tables \| take 1` (~10 lines) |
| `graph-query-api/backends/mock.py` | MODIFY | 5 | Implement `ping()` ‚Äî always ok (~5 lines) |
| `graph-query-api/router_health.py` | CREATE | 5 | `GET /query/health/sources` endpoint (~60 lines) |
| `frontend/src/components/DataSourceBar.tsx` | CREATE | 5 | Horizontal strip rendering `DataSourceCard` per source (~60 lines) |
| `frontend/src/components/DataSourceCard.tsx` | CREATE | 5 | Individual card with green/red dot + hover tooltip showing query (~50 lines) |
| `frontend/src/components/Header.tsx` | MODIFY | 5 | L49: add `<DataSourceBar />` after `<AgentBar />` (~1 line) |
| `graph-query-api/adapters/fabric_config.py` | MODIFY | 6 | Add `fabric_asset_names()` helper (~10 lines) |
| `graph-query-api/config_store.py` | NO CHANGE | ‚Äî | Use existing `save_scenario_config()` ‚Äî no new function needed |

### Files NOT Changed (Verified Correct)

| File | Why No Change |
|------|---------------|
| `graph-query-api/router_graph.py` | Routing is correct ‚Äî dispatches based on `ScenarioContext.backend_type` from config store |
| `graph-query-api/router_telemetry.py` | Routing logic correct ‚Äî issue is in config + upload pipeline, not query dispatch |
| `graph-query-api/ingest/telemetry_ingest.py` | Cosmos NoSQL ingest ‚Äî stays for Cosmos scenarios |
| `scripts/agent_provisioner.py` L53-96 | `CONNECTOR_OPENAPI_VARS` already has correct GQL/KQL descriptions for `"fabric"` key |
| `data/scenarios/telco-noc/` | Cosmos scenario unchanged ‚Äî all fixes scoped to `telco-noc-fabric` |
| `frontend/src/components/AgentBar.tsx` | Visual pattern reference only ‚Äî `DataSourceBar` is a new sibling component |
| `graph-query-api/sse_helpers.py` | SSE event format already correct |
| `nginx.conf` | No new route prefixes needed (`/query/*` already proxied) |

---

## Edge Cases & Validation

### Prompt Composition Edge Case

**Scenario:** `prompt_ingest.py` derives `language_gql.md` from connector `fabric-gql`. If both `language_gremlin.md` and `language_gql.md` exist in the directory, the filter at L240 correctly skips `language_gremlin.md` and includes `language_gql.md`. No conflict.

### Telemetry Connector Change

**Scenario:** Existing telco-noc-fabric uploads have telemetry in Cosmos NoSQL. After changing connector to `fabric-kql`, the **query endpoint** (`POST /query/telemetry`) will look for KQL backend via `ScenarioContext.telemetry_backend_type` (resolved in `config.py` L130-133 from `TELEMETRY_CONNECTOR_MAP`). If Eventhouse isn't provisioned yet, queries will fail.

**Mitigation:** The Fabric telemetry upload (Phase 2) provisions the Eventhouse as part of the upload flow. Users must re-upload telemetry after this change. Add a note in the scenario description or release notes.

### Decoupled Graph/Telemetry Backend Selection

**Scenario:** A scenario uses `fabric-gql` for graph but `cosmosdb-nosql` for telemetry (a valid hybrid config). The upload job now has separate `backend` (graph) and `telemetry_backend` params. The graph upload routes to Fabric, telemetry routes to Cosmos. The query layer (`config.py` L118-136) already resolves graph and telemetry backends independently via `CONNECTOR_TO_BACKEND` and `TELEMETRY_CONNECTOR_MAP` respectively. ‚úÖ

**Frontend concern:** When user selects "Fabric" backend in `AddScenarioModal`, should `telemetry_backend` auto-set to `fabric-kql`? For now, yes ‚Äî default coupling. A future UX could expose separate dropdowns.

### Data Generation Scripts

**Scenario:** Symlinked scripts generate JSON data. Fabric telemetry ingest (`_ingest_kql_data`) expects CSV. The existing pipeline handles this ‚Äî `_ingest_kql_data` in `fabric_provision.py` already reads CSV files from the tarball, not JSON.

**Verification:** Check that `generate_telemetry.py` outputs CSV (it does ‚Äî the telemetry tarball contains `.csv` files).

### Concurrent Backend Scenarios

**Scenario:** `telco-noc` (Cosmos) and `telco-noc-fabric` (Fabric) both exist. Switching between them should route to the correct backends. The config store lookup uses the scenario prefix (`telco-noc` vs `telco-noc-fabric`) so they resolve independently. ‚úÖ

### Fabric Environment Not Configured

**Scenario:** Fabric env vars (`FABRIC_WORKSPACE_ID`, `FABRIC_GRAPH_MODEL_ID`) not set. The upload will fail at the Fabric provisioning step. The error should be clear ‚Äî check that `fabric_provision.py` returns a descriptive error, not a 500.

### Health Check Latency (Item 5)

**Scenario:** A `ping()` call to a slow backend (e.g., cold Fabric endpoint) takes 5+ seconds. The health check endpoint should have a per-source timeout (e.g., 10s) so one slow source doesn't block the entire response. Use `asyncio.wait_for()` or `asyncio.gather()` with timeouts.

**Mitigation:** Run all pings concurrently via `asyncio.gather(*pings, return_exceptions=True)` and set individual timeouts.

### Health Check with No Active Scenario (Item 5)

**Scenario:** No scenario is active. `DataSourceBar` gets no sources from the API. It returns `null` and renders nothing ‚Äî no bar, no height. Identical to AgentBar behavior when `agents.length === 0`.

### Search Index Health Check (Item 5)

**Scenario:** AI Search indexes don't use the `GraphBackend` protocol. The health check endpoint should have a separate path for `search_indexes.*` sources that calls the Azure AI Search management API (`GET /indexes/{name}`). Return `ok: true` if the index exists with a document count > 0.

### Named Asset Collision (Item 6)

**Scenario:** Two scenarios with names that produce the same asset prefix. E.g., scenario `"telco-noc-fabric"` creates `telco-noc-fabric-lakehouse`. A second scenario named `"telco-noc"` with Fabric would want `telco-noc-lakehouse`. No collision ‚Äî names are distinct. However, scenario `"telco noc fabric"` (with spaces) could cause issues. **Mitigation:** Sanitize scenario names to lowercase alphanumeric + hyphens before deriving asset names.

### Per-Scenario Fabric ID Persistence (Item 6)

**Scenario:** `save_scenario_config()` is called after provisioning. If the config store write fails, the provisioned Fabric assets exist but their IDs aren't recorded. **Mitigation:** Log a warning and include the provisioned IDs in the SSE `complete` event so they're visible in upload status.

### Scenario Isolation Race Condition (Item 7)

**Scenario:** Two saves with different backends could both read "no existing doc" and both succeed (last-writer-wins). Mitigation via ETag `if-match` is possible but concurrent scenario creation is extremely unlikely in this single-user demo context. Acceptable risk.

### Scenario with No `graph_connector` Field (Item 7)

**Scenario:** Old scenario docs without `graph_connector` default to `"cosmosdb-gremlin"` in the conflict check, matching the default in `save_scenario()`. Existing data is compatible.

### 409 Response Format (Item 7)

**Scenario:** `HTTPException(409, detail={...})` produces FastAPI response `{"detail": {"message": "...", "suggestion": "..."}}`. Frontend must parse `error.message` to extract the suggestion.

---

## Migration & Backwards Compatibility

### Existing Data

- `telco-noc-fabric` telemetry data currently lives in Cosmos NoSQL containers (`telco-noc-fabric-AlertStream`, `telco-noc-fabric-LinkTelemetry`). After this fix, new uploads will go to Fabric Eventhouse. The Cosmos data is orphaned but harmless.
- Existing `telco-noc` (Cosmos) scenarios are unaffected ‚Äî all changes are scoped to the `telco-noc-fabric` scenario.

### Agent Re-provisioning Required

After updating the prompts:
1. **Re-upload** telco-noc-fabric scenario data (so new prompts are stored)
2. **Re-provision agents** (so the agent tool descriptions reflect KQL, and the composed prompt uses `language_gql.md`)

### No API Breaking Changes

- New endpoint `POST /api/fabric/provision/telemetry` is purely additive
- New endpoint `GET /query/health/sources` is purely additive
- Config store format extended (new `fabric_resources` key) ‚Äî backwards compatible (key absent = old behavior)
- `POST /query/scenarios/save` can now return 409 on cross-backend conflict ‚Äî this is a behavior change, not a schema change; consumers that don't handle 409 will see an error, which is the desired behavior
- `upload_jobs.py` `POST /api/upload-jobs` adds optional `telemetry_backend` Form parameter ‚Äî defaults to `"cosmosdb-nosql"`, backward compatible with existing callers that omit it
- Existing Cosmos scenarios unaffected

### Rollback

- **Phase 1:** Delete `language_gql.md` and `foundry_telemetry_agent_kql.md`. Revert scenario.yaml prompt ref. Agents go back to no-language-instructions behavior (broken but same as before).
- **Phase 2:** Remove `telemetry_backend` param and routing branch. Telemetry goes back to Cosmos NoSQL. Revert scenario.yaml connector.
- **Phase 3:** Remove 409 check in `router_scenarios.py`. Cross-backend overwrites resume. Revert frontend 409 handling.
- **Phase 4:** Remove logger.warning and empty-state div. Silent fallback + blank canvas returns.
- **Phase 5:** Delete `DataSourceBar.tsx`, `DataSourceCard.tsx`, `router_health.py`. Remove `ping()` from backends. Remove `<DataSourceBar />` from Header.tsx. Health strip disappears.
- **Phase 6:** Revert `fabric_config.py`, `fabric_provision.py`, `config.py` changes. Assets go back to generic names. Per-scenario IDs no longer stored.

---

## UX Priority Matrix

| Priority | Phase | Fix | Effort | Impact |
|----------|-------|-----|--------|--------|
| **P0** | 1 | Copy `language_gql.md` to telco-noc-fabric | Tiny | High ‚Äî agents currently get NO query language instructions |
| **P0** | 1 | Create `foundry_telemetry_agent_kql.md` | Medium | High ‚Äî telemetry agent uses Cosmos SQL against Eventhouse |
| **P0** | 2 | Change telemetry connector to `fabric-kql` in scenario.yaml | Tiny | High ‚Äî fixes misrouting at source |
| **P0** | 2 | Add `telemetry_backend` Form param + Fabric telemetry routing in `upload_jobs.py` | Medium | High ‚Äî enables Eventhouse provisioning via upload |
| **P0** | 2 | Create `POST /api/fabric/provision/telemetry` endpoint | Medium | High ‚Äî required by routing change |
| **P0** | 3 | Backend conflict 409 check in `router_scenarios.py` | Small | High ‚Äî prevents silent data corruption |
| **P0** | 3 | Handle 409 in `useScenarioUpload.ts` | Tiny | Medium ‚Äî user sees actionable error message |
| **P1** | 3 | Proactive 409 conflict warning banner in `AddScenarioModal.tsx` | Small | Medium ‚Äî prevents errors before they happen |
| **P1** | 4 | Add logging to `config.py` fallback (L135-136) | Tiny | Medium ‚Äî aids debugging silent misrouting |
| **P1** | 4 | Add empty-state message for `GraphTopologyViewer.tsx` | Tiny | Medium ‚Äî replaces confusing blank screen |
| **P1** | 5 | Data source health check cards ‚Äî `DataSourceBar` + `ping()` | Medium | High ‚Äî visual proof of correct routing |
| **P1** | 6 | Named Fabric assets (`SCENARIO-ontology`, etc.) | Medium | High ‚Äî enables multi-scenario Fabric coexistence |
| **P1** | 6 | Per-scenario Fabric resource ID storage | Small | Medium ‚Äî foundation for multi-Fabric scenarios |
| **P2** | 1 | Symlink data generation scripts | Tiny | Low ‚Äî scripts exist in telco-noc; symlinks for completeness |
| **P2** | ‚Äî | Verify `CONNECTOR_OPENAPI_VARS` for KQL telemetry tool | None | None ‚Äî already verified correct at L53-96 |
