# Alert Storm Triage Guide

## Scope

Guidance for triaging alert storms — high-volume bursts of alerts generated by cascading network failures. This guide focuses on identifying root causes amid noise, with special attention to **compound failures** involving multiple simultaneous root causes.

## Step 1: Temporal Ordering

Sort all alerts by Timestamp (ascending). The earliest alerts are closest to the root cause.

**Key principle:** In a cascading failure, the root cause alert appears FIRST. Everything after it is a downstream consequence. For compound failures, look for **two distinct T+0 clusters** — two separate initial alerts appearing within seconds of each other but affecting different entities or domains.

## Step 2: Noise Suppression

Remove these alert types from your analysis — they are noise:

| AlertType | Why it's noise |
|---|---|
| DUPLICATE_ALERT | Repeated copies of existing alerts |
| PACKET_LOSS_THRESHOLD (MINOR) | Often a downstream effect, not a root cause |

Retain all CRITICAL and MAJOR alerts. WARNING alerts are useful for context but rarely indicate root cause.

## Step 3: Root Cause Identification

### Single root cause pattern
- One entity appears in the earliest alert(s)
- All subsequent alerts are on entities that are topologically downstream
- Example: SUBMARINE_CABLE_FAULT on LINK-X → BGP_PEER_LOSS on connected routers → SERVICE_DEGRADATION on services using that corridor

### Compound failure pattern (TWO root causes)
- **Two distinct initial alerts** appear within 0–15 seconds of each other
- They affect **different entities** in **different failure domains** (e.g., a transport link AND a power system)
- The subsequent cascade is interleaved — alerts from both failure domains appear mixed together
- Example: T+0s SUBMARINE_CABLE_FAULT on LINK-X, T+10s POWER_FAILURE on DC-Y — the cascades overlap

### How to detect compound failures:
1. Look at the FIRST 5 CRITICAL alerts by timestamp.
2. If they involve entities from different geographic or functional domains (e.g., a transport link failure + a datacenter power failure), suspect compound failure.
3. Separate the alert stream into two timelines — one for each suspected root cause domain. Analyse each independently.
4. Assess whether the second failure is truly independent or was caused by the first (e.g., capacity overload from rerouted traffic causing a secondary failure).

## Step 4: Cascade Analysis

For each identified root cause, map the expected cascade sequence:

### Transport failure cascade (submarine cable, fibre cut):
```
T+0s:  SUBMARINE_CABLE_FAULT / FIBRE_CUT (root cause)
T+3s:  BGP_PEER_LOSS (routing session drops)
T+5s:  LINK_FAILOVER (traffic shifts to backup)
T+8s:  CAPACITY_EXCEEDED (if backup is undersized)
T+15s: OSPF_ADJACENCY_DOWN (routing reconvergence)
T+20s: ROUTE_WITHDRAWAL (prefixes withdrawn)
T+25s: HIGH_CPU (convergence pressure)
T+35s: PACKET_LOSS_THRESHOLD (congestion effects)
T+60s: SERVICE_DEGRADATION (customer impact)
T+90s: DUPLICATE_ALERT (noise)
```

### Power failure cascade:
```
T+0s:  POWER_FAILURE (root cause)
T+2s:  BACKHAUL_DOWN (base stations lose connectivity)
T+5s:  OSPF_ADJACENCY_DOWN (if core router affected)
T+10s: SERVICE_DEGRADATION (dependent services impacted)
T+30s: DUPLICATE_ALERT (noise)
```

## Step 5: Correlation Summary

Produce a correlation summary:

1. **Root Cause(s):** The entity ID(s) that appear in the earliest alert(s). For compound failures, list both.
2. **Failure Domain(s):** Geographic or functional scope of each root cause.
3. **Cascade Depth:** How many layers of downstream alerts before SERVICE_DEGRADATION.
4. **Compound Interaction:** How the two failure domains amplify each other (if applicable).
5. **Noise Ratio:** What percentage of total alerts are DUPLICATE_ALERT or MINOR-severity noise.

## Escalation

| Alert Volume | Action |
|---|---|
| > 100 alerts in 5 minutes | Declare alert storm — activate this triage guide |
| > 500 alerts in 5 minutes | Escalate to Network Engineering lead |
| > 1000 alerts with 2+ CRITICAL root causes | Declare compound incident — escalate to VP Operations |

## Cross-References

- See `submarine_cable_runbook.md` for submarine cable root cause response.
- See `power_failure_runbook.md` for power failure root cause response.
- See `bgp_peer_loss_runbook.md` for BGP-specific diagnosis.
- See `capacity_exhaustion_runbook.md` for backup capacity management.
